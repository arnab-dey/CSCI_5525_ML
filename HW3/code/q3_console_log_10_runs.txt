###### Running part 1: SGD with batch size 32 ######
Epoch:  1 , loss =  0.5682076811790466 , accuracy =  0.8182500004768372
Epoch:  2 , loss =  0.2888515591621399 , accuracy =  0.9112833142280579
Epoch:  3 , loss =  0.22527168691158295 , accuracy =  0.930400013923645
Epoch:  4 , loss =  0.19458428025245667 , accuracy =  0.9399333596229553
Epoch:  5 , loss =  0.17121343314647675 , accuracy =  0.9472833275794983
Epoch:  6 , loss =  0.15991663932800293 , accuracy =  0.9500166773796082
Epoch:  7 , loss =  0.14746221899986267 , accuracy =  0.9545333385467529
Epoch:  8 , loss =  0.14007225632667542 , accuracy =  0.9569000005722046
Epoch:  9 , loss =  0.1312740296125412 , accuracy =  0.9590333104133606
Epoch:  10 , loss =  0.12437658756971359 , accuracy =  0.9614333510398865
Epoch:  11 , loss =  0.12117230892181396 , accuracy =  0.9628666639328003
Epoch:  12 , loss =  0.11595924943685532 , accuracy =  0.9643666744232178
Epoch:  13 , loss =  0.11081690341234207 , accuracy =  0.9653000235557556
Epoch:  14 , loss =  0.10801741480827332 , accuracy =  0.9671499729156494
Epoch:  15 , loss =  0.10477583855390549 , accuracy =  0.9670666456222534
Epoch:  16 , loss =  0.10143990814685822 , accuracy =  0.9691500067710876
Epoch:  17 , loss =  0.09884315729141235 , accuracy =  0.9696000218391418
Epoch:  18 , loss =  0.0942196175456047 , accuracy =  0.9705333113670349
Epoch:  19 , loss =  0.09588362276554108 , accuracy =  0.9705333113670349
Epoch:  20 , loss =  0.08948592096567154 , accuracy =  0.9717833399772644
Epoch:  21 , loss =  0.09029644727706909 , accuracy =  0.9719499945640564
Epoch:  22 , loss =  0.08876918256282806 , accuracy =  0.9729833602905273
Epoch:  23 , loss =  0.08373276889324188 , accuracy =  0.9736666679382324
Epoch:  24 , loss =  0.0839889794588089 , accuracy =  0.973800003528595
Epoch:  25 , loss =  0.08343170583248138 , accuracy =  0.9741166830062866
Epoch:  26 , loss =  0.07866982370615005 , accuracy =  0.9753833413124084
Epoch:  27 , loss =  0.07898558676242828 , accuracy =  0.9753999710083008
Epoch:  28 , loss =  0.07830961048603058 , accuracy =  0.9756500124931335
Epoch:  29 , loss =  0.07921059429645538 , accuracy =  0.9748499989509583
Epoch:  30 , loss =  0.07450788468122482 , accuracy =  0.9769166707992554
Epoch:  31 , loss =  0.0728064477443695 , accuracy =  0.9773333072662354
Epoch:  32 , loss =  0.07475080341100693 , accuracy =  0.9768666625022888
Epoch:  33 , loss =  0.0719955787062645 , accuracy =  0.977066695690155
Epoch:  34 , loss =  0.07088860869407654 , accuracy =  0.9778500199317932
Epoch:  35 , loss =  0.06964924186468124 , accuracy =  0.9779166579246521
Epoch:  36 , loss =  0.06919311732053757 , accuracy =  0.9777666926383972
Epoch:  37 , loss =  0.06868591904640198 , accuracy =  0.9784333109855652
Epoch:  38 , loss =  0.066066212952137 , accuracy =  0.9789666533470154
Epoch:  39 , loss =  0.06705635040998459 , accuracy =  0.9783166646957397
Epoch:  40 , loss =  0.06732799112796783 , accuracy =  0.9780833125114441
Epoch:  41 , loss =  0.06316059827804565 , accuracy =  0.9796333312988281
Epoch:  42 , loss =  0.06575183570384979 , accuracy =  0.9789000153541565
Epoch:  43 , loss =  0.06236455962061882 , accuracy =  0.9799333214759827
Epoch:  44 , loss =  0.05927298963069916 , accuracy =  0.9802166819572449
Epoch:  45 , loss =  0.06250344961881638 , accuracy =  0.9805333614349365
Epoch:  46 , loss =  0.059358540922403336 , accuracy =  0.9809666872024536
Epoch:  47 , loss =  0.06008158251643181 , accuracy =  0.9804333448410034
test loss =  0.032648585736751556 , test accuracy =  0.989300012588501
###### RUN NUMBER  0  ######
###### Running part 2: SGD with batch size  32  ######
Epoch:  1 , loss =  0.5344163179397583 , accuracy =  0.8281499743461609
Epoch:  2 , loss =  0.2694893181324005 , accuracy =  0.9174333214759827
Epoch:  3 , loss =  0.2177632451057434 , accuracy =  0.933899998664856
Epoch:  4 , loss =  0.18710385262966156 , accuracy =  0.9429833292961121
Epoch:  5 , loss =  0.16487225890159607 , accuracy =  0.9501000046730042
Epoch:  6 , loss =  0.1557261347770691 , accuracy =  0.9523333311080933
Epoch:  7 , loss =  0.14588019251823425 , accuracy =  0.9561833143234253
Epoch:  8 , loss =  0.1355600208044052 , accuracy =  0.9587833285331726
Epoch:  9 , loss =  0.12991246581077576 , accuracy =  0.9605000019073486
Epoch:  10 , loss =  0.12065551429986954 , accuracy =  0.9631999731063843
Epoch:  11 , loss =  0.11950306594371796 , accuracy =  0.9631333351135254
Epoch:  12 , loss =  0.11293766647577286 , accuracy =  0.9647666811943054
Epoch:  13 , loss =  0.10665330290794373 , accuracy =  0.9668999910354614
Epoch:  14 , loss =  0.10438795387744904 , accuracy =  0.9675499796867371
Epoch:  15 , loss =  0.09946198016405106 , accuracy =  0.968500018119812
Epoch:  16 , loss =  0.0986543595790863 , accuracy =  0.9692500233650208
Epoch:  17 , loss =  0.09652778506278992 , accuracy =  0.9703500270843506
Epoch:  18 , loss =  0.09357742220163345 , accuracy =  0.9713166952133179
Epoch:  19 , loss =  0.09170138835906982 , accuracy =  0.9716333150863647
Epoch:  20 , loss =  0.0897488072514534 , accuracy =  0.9723666906356812
Epoch:  21 , loss =  0.08673379570245743 , accuracy =  0.9727833271026611
Epoch:  22 , loss =  0.08594194054603577 , accuracy =  0.973800003528595
Epoch:  23 , loss =  0.0822434052824974 , accuracy =  0.9739000201225281
Epoch:  24 , loss =  0.08259505778551102 , accuracy =  0.9740833044052124
Epoch:  25 , loss =  0.07946740835905075 , accuracy =  0.9758999943733215
Epoch:  26 , loss =  0.07752971351146698 , accuracy =  0.9751999974250793
Epoch:  27 , loss =  0.07894381880760193 , accuracy =  0.9754499793052673
Epoch:  28 , loss =  0.07513778656721115 , accuracy =  0.9761666655540466
Epoch:  29 , loss =  0.07535760849714279 , accuracy =  0.9761833548545837
Epoch:  30 , loss =  0.07515479624271393 , accuracy =  0.9766833186149597
Epoch:  31 , loss =  0.07376468926668167 , accuracy =  0.9770166873931885
Epoch:  32 , loss =  0.07136429101228714 , accuracy =  0.9774666428565979
Epoch:  33 , loss =  0.07150624692440033 , accuracy =  0.9775500297546387
Epoch:  34 , loss =  0.07014898955821991 , accuracy =  0.9783833622932434
Epoch:  35 , loss =  0.06800077855587006 , accuracy =  0.9784833192825317
Epoch:  36 , loss =  0.06718339025974274 , accuracy =  0.9789166450500488
Epoch:  37 , loss =  0.06596667319536209 , accuracy =  0.9792333245277405
Epoch:  38 , loss =  0.06672095507383347 , accuracy =  0.979033350944519
Epoch:  39 , loss =  0.06464827060699463 , accuracy =  0.9796833395957947
Epoch:  40 , loss =  0.06482064723968506 , accuracy =  0.9789833426475525
Epoch:  41 , loss =  0.06309573352336884 , accuracy =  0.9800666570663452
Epoch:  42 , loss =  0.06247318908572197 , accuracy =  0.9808833599090576
Epoch:  43 , loss =  0.0631173849105835 , accuracy =  0.9795166850090027
Epoch:  44 , loss =  0.061525970697402954 , accuracy =  0.9797666668891907
Epoch:  45 , loss =  0.06175189092755318 , accuracy =  0.9805999994277954
Epoch:  46 , loss =  0.06049194559454918 , accuracy =  0.9807999730110168
Epoch:  47 , loss =  0.05895090848207474 , accuracy =  0.9811000227928162
Epoch:  48 , loss =  0.05729693919420242 , accuracy =  0.9814833402633667
Epoch:  49 , loss =  0.057463183999061584 , accuracy =  0.9814500212669373
Epoch:  50 , loss =  0.05886898189783096 , accuracy =  0.9815999865531921
Epoch:  51 , loss =  0.05850799381732941 , accuracy =  0.9815166592597961
SGD: run no. =  0 , batch size =  32  convergence time =  165.43344235420227
SGD: run no. =  0 , batch size =  32 , test loss =  0.03046930767595768 , test accuracy =  0.9891999959945679
###### Running part 2: ADAGRAD with batch size  32  ######
Epoch:  1 , loss =  1.0024107694625854 , accuracy =  0.6694166660308838
Epoch:  2 , loss =  0.3413979113101959 , accuracy =  0.8944666385650635
Epoch:  3 , loss =  0.26363247632980347 , accuracy =  0.9202333092689514
Epoch:  4 , loss =  0.22598548233509064 , accuracy =  0.9333000183105469
Epoch:  5 , loss =  0.20606642961502075 , accuracy =  0.937583327293396
Epoch:  6 , loss =  0.19275858998298645 , accuracy =  0.94118332862854
Epoch:  7 , loss =  0.1818801611661911 , accuracy =  0.9451000094413757
Epoch:  8 , loss =  0.17268063127994537 , accuracy =  0.9482166767120361
Epoch:  9 , loss =  0.16132867336273193 , accuracy =  0.9513333439826965
Epoch:  10 , loss =  0.1525339037179947 , accuracy =  0.953249990940094
Epoch:  11 , loss =  0.14636577665805817 , accuracy =  0.9548333287239075
Epoch:  12 , loss =  0.14497144520282745 , accuracy =  0.955216646194458
Epoch:  13 , loss =  0.13743191957473755 , accuracy =  0.9583166837692261
Epoch:  14 , loss =  0.13098855316638947 , accuracy =  0.9605166912078857
Epoch:  15 , loss =  0.1315966099500656 , accuracy =  0.9603166580200195
Epoch:  16 , loss =  0.12976080179214478 , accuracy =  0.9600499868392944
Epoch:  17 , loss =  0.1293335109949112 , accuracy =  0.960433304309845
Epoch:  18 , loss =  0.1240769475698471 , accuracy =  0.9622166752815247
Epoch:  19 , loss =  0.12401452660560608 , accuracy =  0.9630333185195923
Epoch:  20 , loss =  0.11984962224960327 , accuracy =  0.9640666842460632
Epoch:  21 , loss =  0.11862217634916306 , accuracy =  0.9637166857719421
Epoch:  22 , loss =  0.11609558016061783 , accuracy =  0.9638333320617676
Epoch:  23 , loss =  0.11332990974187851 , accuracy =  0.9649333357810974
Epoch:  24 , loss =  0.1166861355304718 , accuracy =  0.9643499851226807
Epoch:  25 , loss =  0.11239312589168549 , accuracy =  0.9656999707221985
Epoch:  26 , loss =  0.10922873765230179 , accuracy =  0.9664499759674072
Epoch:  27 , loss =  0.1074361577630043 , accuracy =  0.9678333401679993
Epoch:  28 , loss =  0.10789016634225845 , accuracy =  0.9674666523933411
Epoch:  29 , loss =  0.10659435391426086 , accuracy =  0.96711665391922
Epoch:  30 , loss =  0.1043231189250946 , accuracy =  0.9687833189964294
Epoch:  31 , loss =  0.10313291847705841 , accuracy =  0.968833327293396
Epoch:  32 , loss =  0.10198011994361877 , accuracy =  0.9683499932289124
Epoch:  33 , loss =  0.09958193451166153 , accuracy =  0.9682666659355164
Epoch:  34 , loss =  0.10183293372392654 , accuracy =  0.9692333340644836
Epoch:  35 , loss =  0.09936832636594772 , accuracy =  0.9690333604812622
Epoch:  36 , loss =  0.09798596054315567 , accuracy =  0.9700999855995178
Epoch:  37 , loss =  0.09932176768779755 , accuracy =  0.9689499735832214
Epoch:  38 , loss =  0.0949988141655922 , accuracy =  0.9705833196640015
Epoch:  39 , loss =  0.0945098027586937 , accuracy =  0.9708666801452637
Epoch:  40 , loss =  0.09610583633184433 , accuracy =  0.9701166749000549
Epoch:  41 , loss =  0.09475015103816986 , accuracy =  0.9711333513259888
Epoch:  42 , loss =  0.09484072029590607 , accuracy =  0.9700833559036255
ADAGRAD: run no. =  0 , batch size =  32  convergence time =  140.26688933372498
ADAGRAD: run no. =  0 , batch size =  32 , test loss =  0.041859086602926254 , test accuracy =  0.9857000112533569
###### Running part 2: ADAM with batch size  32  ######
Epoch:  1 , loss =  0.418780118227005 , accuracy =  0.8686666488647461
Epoch:  2 , loss =  0.19875122606754303 , accuracy =  0.9386666417121887
Epoch:  3 , loss =  0.1618547886610031 , accuracy =  0.9507166743278503
Epoch:  4 , loss =  0.14324277639389038 , accuracy =  0.956250011920929
Epoch:  5 , loss =  0.13037452101707458 , accuracy =  0.9603999853134155
Epoch:  6 , loss =  0.11689598858356476 , accuracy =  0.9643999934196472
Epoch:  7 , loss =  0.10932580381631851 , accuracy =  0.9674666523933411
Epoch:  8 , loss =  0.10211899131536484 , accuracy =  0.9686499834060669
Epoch:  9 , loss =  0.09906087815761566 , accuracy =  0.9699666500091553
Epoch:  10 , loss =  0.09094658493995667 , accuracy =  0.9720166921615601
Epoch:  11 , loss =  0.09037375450134277 , accuracy =  0.9721666574478149
Epoch:  12 , loss =  0.08364732563495636 , accuracy =  0.9744333624839783
Epoch:  13 , loss =  0.08462201058864594 , accuracy =  0.9734833240509033
Epoch:  14 , loss =  0.08111072331666946 , accuracy =  0.9752500057220459
Epoch:  15 , loss =  0.07736925780773163 , accuracy =  0.9753333330154419
Epoch:  16 , loss =  0.07832453399896622 , accuracy =  0.9760500192642212
Epoch:  17 , loss =  0.07351235300302505 , accuracy =  0.9771833419799805
Epoch:  18 , loss =  0.07331683486700058 , accuracy =  0.9761000275611877
Epoch:  19 , loss =  0.07080164551734924 , accuracy =  0.9782166481018066
Epoch:  20 , loss =  0.0695156678557396 , accuracy =  0.9784666895866394
Epoch:  21 , loss =  0.07023759931325912 , accuracy =  0.9782000184059143
Epoch:  22 , loss =  0.06771411746740341 , accuracy =  0.9791499972343445
Epoch:  23 , loss =  0.06525803357362747 , accuracy =  0.9795833230018616
Epoch:  24 , loss =  0.0659293681383133 , accuracy =  0.9793499708175659
Epoch:  25 , loss =  0.06136699393391609 , accuracy =  0.9807000160217285
Epoch:  26 , loss =  0.06482122838497162 , accuracy =  0.9796833395957947
Epoch:  27 , loss =  0.06444140523672104 , accuracy =  0.9796333312988281
Epoch:  28 , loss =  0.060900378972291946 , accuracy =  0.9807999730110168
Epoch:  29 , loss =  0.06351052969694138 , accuracy =  0.9800333380699158
Epoch:  30 , loss =  0.06330879032611847 , accuracy =  0.9800333380699158
Epoch:  31 , loss =  0.059203580021858215 , accuracy =  0.981416642665863
Epoch:  32 , loss =  0.05823943391442299 , accuracy =  0.9814833402633667
Epoch:  33 , loss =  0.059844452887773514 , accuracy =  0.9810333251953125
Epoch:  34 , loss =  0.058967798948287964 , accuracy =  0.9816833138465881
Epoch:  35 , loss =  0.05809043347835541 , accuracy =  0.9820333123207092
Epoch:  36 , loss =  0.057552363723516464 , accuracy =  0.9825000166893005
ADAM: run no. =  0 , batch size =  32  convergence time =  122.41136932373047
ADAM: run no. =  0 , batch size =  32 , test loss =  0.040187589824199677 , test accuracy =  0.9878000020980835
###### Running part 2: SGD with batch size  64  ######
Epoch:  1 , loss =  0.05876926705241203 , accuracy =  0.9811499714851379
Epoch:  2 , loss =  0.053794294595718384 , accuracy =  0.9830666780471802
Epoch:  3 , loss =  0.05602677911520004 , accuracy =  0.9817833304405212
Epoch:  4 , loss =  0.054871104657649994 , accuracy =  0.982450008392334
Epoch:  5 , loss =  0.05483071878552437 , accuracy =  0.9821000099182129
SGD: run no. =  0 , batch size =  64  convergence time =  11.647857666015625
SGD: run no. =  0 , batch size =  64 , test loss =  0.030432933941483498 , test accuracy =  0.989799976348877
###### Running part 2: ADAGRAD with batch size  64  ######
Epoch:  1 , loss =  0.08884909003973007 , accuracy =  0.972000002861023
Epoch:  2 , loss =  0.09005171060562134 , accuracy =  0.9715333580970764
Epoch:  3 , loss =  0.09089994430541992 , accuracy =  0.9718000292778015
Epoch:  4 , loss =  0.08599229156970978 , accuracy =  0.9730333089828491
Epoch:  5 , loss =  0.08771584928035736 , accuracy =  0.9723666906356812
Epoch:  6 , loss =  0.08927535265684128 , accuracy =  0.972083330154419
Epoch:  7 , loss =  0.08587401360273361 , accuracy =  0.972683310508728
Epoch:  8 , loss =  0.08757904171943665 , accuracy =  0.9732000231742859
Epoch:  9 , loss =  0.08393588662147522 , accuracy =  0.9739500284194946
Epoch:  10 , loss =  0.08287566155195236 , accuracy =  0.9743166565895081
Epoch:  11 , loss =  0.08393850177526474 , accuracy =  0.9736666679382324
Epoch:  12 , loss =  0.08767084032297134 , accuracy =  0.9730166792869568
Epoch:  13 , loss =  0.08480259776115417 , accuracy =  0.9726166725158691
ADAGRAD: run no. =  0 , batch size =  64  convergence time =  30.63316011428833
ADAGRAD: run no. =  0 , batch size =  64 , test loss =  0.04002855345606804 , test accuracy =  0.9872999787330627
###### Running part 2: ADAM with batch size  64  ######
Epoch:  1 , loss =  0.04993297532200813 , accuracy =  0.9839500188827515
Epoch:  2 , loss =  0.051106784492731094 , accuracy =  0.9838333129882812
Epoch:  3 , loss =  0.04785054922103882 , accuracy =  0.9851333498954773
Epoch:  4 , loss =  0.049455370754003525 , accuracy =  0.9841499924659729
Epoch:  5 , loss =  0.04875895753502846 , accuracy =  0.9838500022888184
Epoch:  6 , loss =  0.05014371871948242 , accuracy =  0.9842333197593689
ADAM: run no. =  0 , batch size =  64  convergence time =  14.498278617858887
ADAM: run no. =  0 , batch size =  64 , test loss =  0.03976692259311676 , test accuracy =  0.9886000156402588
###### Running part 2: SGD with batch size  96  ######
Epoch:  1 , loss =  0.0527973398566246 , accuracy =  0.9831666946411133
Epoch:  2 , loss =  0.05332064628601074 , accuracy =  0.9829666614532471
Epoch:  3 , loss =  0.05413616821169853 , accuracy =  0.9829999804496765
Epoch:  4 , loss =  0.05390388146042824 , accuracy =  0.9827166795730591
SGD: run no. =  0 , batch size =  96  convergence time =  6.543407201766968
SGD: run no. =  0 , batch size =  96 , test loss =  0.0300128273665905 , test accuracy =  0.9901000261306763
###### Running part 2: ADAGRAD with batch size  96  ######
Epoch:  1 , loss =  0.08384045958518982 , accuracy =  0.9738666415214539
Epoch:  2 , loss =  0.08608701080083847 , accuracy =  0.9732499718666077
Epoch:  3 , loss =  0.08241560310125351 , accuracy =  0.9748499989509583
Epoch:  4 , loss =  0.08188910782337189 , accuracy =  0.9740833044052124
Epoch:  5 , loss =  0.08163363486528397 , accuracy =  0.9746500253677368
Epoch:  6 , loss =  0.08198139816522598 , accuracy =  0.9744666814804077
Epoch:  7 , loss =  0.07993027567863464 , accuracy =  0.9744833111763
Epoch:  8 , loss =  0.08083923906087875 , accuracy =  0.9746999740600586
Epoch:  9 , loss =  0.07919081300497055 , accuracy =  0.9746166467666626
Epoch:  10 , loss =  0.0798431858420372 , accuracy =  0.9744166731834412
Epoch:  11 , loss =  0.07921022176742554 , accuracy =  0.974399983882904
Epoch:  12 , loss =  0.07852394133806229 , accuracy =  0.9749333262443542
Epoch:  13 , loss =  0.07917387783527374 , accuracy =  0.975600004196167
Epoch:  14 , loss =  0.07790929824113846 , accuracy =  0.9750499725341797
ADAGRAD: run no. =  0 , batch size =  96  convergence time =  23.424707412719727
ADAGRAD: run no. =  0 , batch size =  96 , test loss =  0.039812833070755005 , test accuracy =  0.9872999787330627
###### Running part 2: ADAM with batch size  96  ######
Epoch:  1 , loss =  0.04835665598511696 , accuracy =  0.9841333627700806
Epoch:  2 , loss =  0.045479916036129 , accuracy =  0.9857000112533569
Epoch:  3 , loss =  0.04727791249752045 , accuracy =  0.9853833317756653
Epoch:  4 , loss =  0.04756418988108635 , accuracy =  0.9850333333015442
Epoch:  5 , loss =  0.045264068990945816 , accuracy =  0.9853333234786987
Epoch:  6 , loss =  0.04461890831589699 , accuracy =  0.9858333468437195
Epoch:  7 , loss =  0.04624858871102333 , accuracy =  0.9850999712944031
Epoch:  8 , loss =  0.04704072326421738 , accuracy =  0.9854333400726318
Epoch:  9 , loss =  0.047061432152986526 , accuracy =  0.984666645526886
ADAM: run no. =  0 , batch size =  96  convergence time =  15.228004455566406
ADAM: run no. =  0 , batch size =  96 , test loss =  0.03572593629360199 , test accuracy =  0.9900000095367432
###### Running part 2: SGD with batch size  128  ######
Epoch:  1 , loss =  0.05284949019551277 , accuracy =  0.9830833077430725
Epoch:  2 , loss =  0.052261002361774445 , accuracy =  0.9830666780471802
Epoch:  3 , loss =  0.0543237030506134 , accuracy =  0.9822666645050049
Epoch:  4 , loss =  0.05212587118148804 , accuracy =  0.9834666848182678
Epoch:  5 , loss =  0.05367359519004822 , accuracy =  0.982783317565918
Epoch:  6 , loss =  0.05283645540475845 , accuracy =  0.9827166795730591
Epoch:  7 , loss =  0.05252831056714058 , accuracy =  0.9829833507537842
SGD: run no. =  0 , batch size =  128  convergence time =  9.121333599090576
SGD: run no. =  0 , batch size =  128 , test loss =  0.029240652918815613 , test accuracy =  0.9901000261306763
###### Running part 2: ADAGRAD with batch size  128  ######
Epoch:  1 , loss =  0.07412171363830566 , accuracy =  0.9765999913215637
Epoch:  2 , loss =  0.07401380687952042 , accuracy =  0.9766499996185303
Epoch:  3 , loss =  0.07442191243171692 , accuracy =  0.9770333170890808
Epoch:  4 , loss =  0.07694894075393677 , accuracy =  0.9758333563804626
Epoch:  5 , loss =  0.07683195173740387 , accuracy =  0.9751999974250793
ADAGRAD: run no. =  0 , batch size =  128  convergence time =  6.7073893547058105
ADAGRAD: run no. =  0 , batch size =  128 , test loss =  0.03945378214120865 , test accuracy =  0.9868999719619751
###### Running part 2: ADAM with batch size  128  ######
Epoch:  1 , loss =  0.043267179280519485 , accuracy =  0.98621666431427
Epoch:  2 , loss =  0.04110180586576462 , accuracy =  0.986549973487854
Epoch:  3 , loss =  0.04174816608428955 , accuracy =  0.9862333536148071
Epoch:  4 , loss =  0.041838452219963074 , accuracy =  0.9866666793823242
Epoch:  5 , loss =  0.04179193824529648 , accuracy =  0.9864833354949951
ADAM: run no. =  0 , batch size =  128  convergence time =  6.820616006851196
ADAM: run no. =  0 , batch size =  128 , test loss =  0.0370294451713562 , test accuracy =  0.9890000224113464
###### RUN NUMBER  1  ######
###### Running part 2: SGD with batch size  32  ######
Epoch:  1 , loss =  0.05650404840707779 , accuracy =  0.9821333289146423
Epoch:  2 , loss =  0.057742245495319366 , accuracy =  0.9818500280380249
Epoch:  3 , loss =  0.05642778426408768 , accuracy =  0.9817333221435547
Epoch:  4 , loss =  0.055048082023859024 , accuracy =  0.982283353805542
Epoch:  5 , loss =  0.054743167012929916 , accuracy =  0.9827166795730591
Epoch:  6 , loss =  0.05384397879242897 , accuracy =  0.982699990272522
Epoch:  7 , loss =  0.055662114173173904 , accuracy =  0.9822666645050049
Epoch:  8 , loss =  0.053725626319646835 , accuracy =  0.9823333621025085
SGD: run no. =  1 , batch size =  32  convergence time =  25.71945333480835
SGD: run no. =  1 , batch size =  32 , test loss =  0.029812097549438477 , test accuracy =  0.9902999997138977
###### Running part 2: ADAGRAD with batch size  32  ######
Epoch:  1 , loss =  0.08010608702898026 , accuracy =  0.9745500087738037
Epoch:  2 , loss =  0.08134981244802475 , accuracy =  0.9751833081245422
Epoch:  3 , loss =  0.0800621435046196 , accuracy =  0.9754166603088379
Epoch:  4 , loss =  0.07956064492464066 , accuracy =  0.9749666452407837
Epoch:  5 , loss =  0.078420490026474 , accuracy =  0.9753833413124084
Epoch:  6 , loss =  0.07931487262248993 , accuracy =  0.9747166633605957
Epoch:  7 , loss =  0.07657019048929214 , accuracy =  0.975683331489563
Epoch:  8 , loss =  0.07558949291706085 , accuracy =  0.9765499830245972
Epoch:  9 , loss =  0.07621382921934128 , accuracy =  0.9766666889190674
Epoch:  10 , loss =  0.07589471340179443 , accuracy =  0.9761333465576172
Epoch:  11 , loss =  0.07708457112312317 , accuracy =  0.9756666421890259
ADAGRAD: run no. =  1 , batch size =  32  convergence time =  36.95970606803894
ADAGRAD: run no. =  1 , batch size =  32 , test loss =  0.039766836911439896 , test accuracy =  0.9871000051498413
###### Running part 2: ADAM with batch size  32  ######
Epoch:  1 , loss =  0.05337277427315712 , accuracy =  0.982866644859314
Epoch:  2 , loss =  0.04954640939831734 , accuracy =  0.9836999773979187
Epoch:  3 , loss =  0.0508715845644474 , accuracy =  0.9837666749954224
Epoch:  4 , loss =  0.048859644681215286 , accuracy =  0.9846000075340271
Epoch:  5 , loss =  0.05328347161412239 , accuracy =  0.9838166832923889
Epoch:  6 , loss =  0.052577681839466095 , accuracy =  0.9837333559989929
Epoch:  7 , loss =  0.050376858562231064 , accuracy =  0.9842333197593689
ADAM: run no. =  1 , batch size =  32  convergence time =  24.3334321975708
ADAM: run no. =  1 , batch size =  32 , test loss =  0.03857109323143959 , test accuracy =  0.9896000027656555
###### Running part 2: SGD with batch size  64  ######
Epoch:  1 , loss =  0.051148246973752975 , accuracy =  0.9834333062171936
Epoch:  2 , loss =  0.05074266716837883 , accuracy =  0.9839833378791809
Epoch:  3 , loss =  0.05128733813762665 , accuracy =  0.9839666485786438
Epoch:  4 , loss =  0.04956123232841492 , accuracy =  0.9838333129882812
Epoch:  5 , loss =  0.04940455034375191 , accuracy =  0.9840499758720398
Epoch:  6 , loss =  0.04905862361192703 , accuracy =  0.9841833114624023
Epoch:  7 , loss =  0.050228625535964966 , accuracy =  0.9842166900634766
Epoch:  8 , loss =  0.048928361386060715 , accuracy =  0.9837666749954224
SGD: run no. =  1 , batch size =  64  convergence time =  18.27376699447632
SGD: run no. =  1 , batch size =  64 , test loss =  0.028790375217795372 , test accuracy =  0.9904000163078308
###### Running part 2: ADAGRAD with batch size  64  ######
Epoch:  1 , loss =  0.07563561946153641 , accuracy =  0.9762666821479797
Epoch:  2 , loss =  0.07681787759065628 , accuracy =  0.9760000109672546
Epoch:  3 , loss =  0.0749794989824295 , accuracy =  0.9765333533287048
Epoch:  4 , loss =  0.07459335774183273 , accuracy =  0.9765333533287048
Epoch:  5 , loss =  0.07273173332214355 , accuracy =  0.9770500063896179
Epoch:  6 , loss =  0.0737854391336441 , accuracy =  0.9768000245094299
Epoch:  7 , loss =  0.07289749383926392 , accuracy =  0.9768166542053223
Epoch:  8 , loss =  0.07208205759525299 , accuracy =  0.9774666428565979
Epoch:  9 , loss =  0.0718444362282753 , accuracy =  0.9778666496276855
Epoch:  10 , loss =  0.07023800909519196 , accuracy =  0.9777166843414307
Epoch:  11 , loss =  0.07020783424377441 , accuracy =  0.9775000214576721
Epoch:  12 , loss =  0.07217463850975037 , accuracy =  0.9770333170890808
Epoch:  13 , loss =  0.07090907543897629 , accuracy =  0.9775333404541016
Epoch:  14 , loss =  0.0715448185801506 , accuracy =  0.9776333570480347
ADAGRAD: run no. =  1 , batch size =  64  convergence time =  32.82683038711548
ADAGRAD: run no. =  1 , batch size =  64 , test loss =  0.038453102111816406 , test accuracy =  0.9876999855041504
###### Running part 2: ADAM with batch size  64  ######
Epoch:  1 , loss =  0.04534289240837097 , accuracy =  0.9859333038330078
Epoch:  2 , loss =  0.04658151790499687 , accuracy =  0.9851833581924438
Epoch:  3 , loss =  0.04763263091444969 , accuracy =  0.9848999977111816
Epoch:  4 , loss =  0.04539027065038681 , accuracy =  0.98621666431427
ADAM: run no. =  1 , batch size =  64  convergence time =  9.535448551177979
ADAM: run no. =  1 , batch size =  64 , test loss =  0.036330439150333405 , test accuracy =  0.9890999794006348
###### Running part 2: SGD with batch size  96  ######
Epoch:  1 , loss =  0.04876315966248512 , accuracy =  0.984666645526886
Epoch:  2 , loss =  0.04924093186855316 , accuracy =  0.9839666485786438
Epoch:  3 , loss =  0.04688968136906624 , accuracy =  0.9850333333015442
Epoch:  4 , loss =  0.04771487042307854 , accuracy =  0.9844499826431274
Epoch:  5 , loss =  0.046497926115989685 , accuracy =  0.985450029373169
Epoch:  6 , loss =  0.04610273614525795 , accuracy =  0.9855499863624573
Epoch:  7 , loss =  0.0474228635430336 , accuracy =  0.9849500060081482
Epoch:  8 , loss =  0.04633970931172371 , accuracy =  0.9847999811172485
Epoch:  9 , loss =  0.045966822654008865 , accuracy =  0.9850166440010071
SGD: run no. =  1 , batch size =  96  convergence time =  14.73281717300415
SGD: run no. =  1 , batch size =  96 , test loss =  0.028404345735907555 , test accuracy =  0.9904999732971191
###### Running part 2: ADAGRAD with batch size  96  ######
Epoch:  1 , loss =  0.07117033749818802 , accuracy =  0.9769333600997925
Epoch:  2 , loss =  0.06950807571411133 , accuracy =  0.9775833487510681
Epoch:  3 , loss =  0.069479800760746 , accuracy =  0.9781500101089478
Epoch:  4 , loss =  0.06756839156150818 , accuracy =  0.9789333343505859
Epoch:  5 , loss =  0.06673138588666916 , accuracy =  0.9783833622932434
Epoch:  6 , loss =  0.06896120309829712 , accuracy =  0.978600025177002
Epoch:  7 , loss =  0.0703601986169815 , accuracy =  0.9777166843414307
Epoch:  8 , loss =  0.06819326430559158 , accuracy =  0.9785166382789612
ADAGRAD: run no. =  1 , batch size =  96  convergence time =  13.24405312538147
ADAGRAD: run no. =  1 , batch size =  96 , test loss =  0.036741942167282104 , test accuracy =  0.9879999756813049
###### Running part 2: ADAM with batch size  96  ######
Epoch:  1 , loss =  0.04590959474444389 , accuracy =  0.9853166937828064
Epoch:  2 , loss =  0.04356282576918602 , accuracy =  0.9861500263214111
Epoch:  3 , loss =  0.04231752082705498 , accuracy =  0.9866333603858948
Epoch:  4 , loss =  0.04302187263965607 , accuracy =  0.9864333271980286
Epoch:  5 , loss =  0.043450117111206055 , accuracy =  0.9866666793823242
Epoch:  6 , loss =  0.041699476540088654 , accuracy =  0.9863166809082031
Epoch:  7 , loss =  0.04376635700464249 , accuracy =  0.9861999750137329
Epoch:  8 , loss =  0.04308813810348511 , accuracy =  0.9865666627883911
Epoch:  9 , loss =  0.04225834459066391 , accuracy =  0.9860333204269409
ADAM: run no. =  1 , batch size =  96  convergence time =  15.093616724014282
ADAM: run no. =  1 , batch size =  96 , test loss =  0.03731181472539902 , test accuracy =  0.989300012588501
###### Running part 2: SGD with batch size  128  ######
Epoch:  1 , loss =  0.045406460762023926 , accuracy =  0.9849166870117188
Epoch:  2 , loss =  0.04475590959191322 , accuracy =  0.9855833053588867
Epoch:  3 , loss =  0.04486232250928879 , accuracy =  0.9850666522979736
Epoch:  4 , loss =  0.04398486390709877 , accuracy =  0.9857666492462158
Epoch:  5 , loss =  0.04517437517642975 , accuracy =  0.9852833151817322
Epoch:  6 , loss =  0.046474091708660126 , accuracy =  0.9850500226020813
Epoch:  7 , loss =  0.04465893656015396 , accuracy =  0.9856500029563904
SGD: run no. =  1 , batch size =  128  convergence time =  9.104392290115356
SGD: run no. =  1 , batch size =  128 , test loss =  0.028846029192209244 , test accuracy =  0.9907000064849854
###### Running part 2: ADAGRAD with batch size  128  ######
Epoch:  1 , loss =  0.06949212402105331 , accuracy =  0.9781500101089478
Epoch:  2 , loss =  0.06926649808883667 , accuracy =  0.9778333306312561
Epoch:  3 , loss =  0.06748925894498825 , accuracy =  0.9789000153541565
Epoch:  4 , loss =  0.06505323201417923 , accuracy =  0.9789166450500488
Epoch:  5 , loss =  0.0679718554019928 , accuracy =  0.9784500002861023
Epoch:  6 , loss =  0.06623846292495728 , accuracy =  0.9801333546638489
Epoch:  7 , loss =  0.06793775409460068 , accuracy =  0.9782999753952026
ADAGRAD: run no. =  1 , batch size =  128  convergence time =  9.278527021408081
ADAGRAD: run no. =  1 , batch size =  128 , test loss =  0.03789384290575981 , test accuracy =  0.9882000088691711
###### Running part 2: ADAM with batch size  128  ######
Epoch:  1 , loss =  0.04259199649095535 , accuracy =  0.9865833520889282
Epoch:  2 , loss =  0.04040287807583809 , accuracy =  0.9870666861534119
Epoch:  3 , loss =  0.03976348042488098 , accuracy =  0.9870666861534119
Epoch:  4 , loss =  0.03935297951102257 , accuracy =  0.9872999787330627
Epoch:  5 , loss =  0.041171394288539886 , accuracy =  0.9873666763305664
Epoch:  6 , loss =  0.03987668827176094 , accuracy =  0.987416684627533
Epoch:  7 , loss =  0.037063419818878174 , accuracy =  0.9879666566848755
Epoch:  8 , loss =  0.03809591010212898 , accuracy =  0.9879833459854126
Epoch:  9 , loss =  0.03807992488145828 , accuracy =  0.9872999787330627
Epoch:  10 , loss =  0.03948614001274109 , accuracy =  0.9868666529655457
ADAM: run no. =  1 , batch size =  128  convergence time =  13.464368104934692
ADAM: run no. =  1 , batch size =  128 , test loss =  0.03762868046760559 , test accuracy =  0.989300012588501
###### RUN NUMBER  2  ######
###### Running part 2: SGD with batch size  32  ######
Epoch:  1 , loss =  0.04613957554101944 , accuracy =  0.9852666854858398
Epoch:  2 , loss =  0.050820764154195786 , accuracy =  0.9838500022888184
Epoch:  3 , loss =  0.04767661169171333 , accuracy =  0.9850500226020813
Epoch:  4 , loss =  0.04937531426548958 , accuracy =  0.9840166568756104
SGD: run no. =  2 , batch size =  32  convergence time =  12.992065906524658
SGD: run no. =  2 , batch size =  32 , test loss =  0.028251104056835175 , test accuracy =  0.9908000230789185
###### Running part 2: ADAGRAD with batch size  32  ######
Epoch:  1 , loss =  0.07230918854475021 , accuracy =  0.9774333238601685
Epoch:  2 , loss =  0.07243580371141434 , accuracy =  0.9775333404541016
Epoch:  3 , loss =  0.07070842385292053 , accuracy =  0.9772666692733765
Epoch:  4 , loss =  0.06924217194318771 , accuracy =  0.9785333275794983
Epoch:  5 , loss =  0.07104466110467911 , accuracy =  0.9772833585739136
Epoch:  6 , loss =  0.0714956670999527 , accuracy =  0.9773833155632019
Epoch:  7 , loss =  0.06947137415409088 , accuracy =  0.9784500002861023
ADAGRAD: run no. =  2 , batch size =  32  convergence time =  23.397685050964355
ADAGRAD: run no. =  2 , batch size =  32 , test loss =  0.038897670805454254 , test accuracy =  0.9876000285148621
###### Running part 2: ADAM with batch size  32  ######
Epoch:  1 , loss =  0.04920192435383797 , accuracy =  0.9847333431243896
Epoch:  2 , loss =  0.04689691960811615 , accuracy =  0.9854333400726318
Epoch:  3 , loss =  0.047182708978652954 , accuracy =  0.9848999977111816
Epoch:  4 , loss =  0.04622102528810501 , accuracy =  0.9853000044822693
Epoch:  5 , loss =  0.049984563142061234 , accuracy =  0.9850999712944031
Epoch:  6 , loss =  0.05000551789999008 , accuracy =  0.9848833084106445
Epoch:  7 , loss =  0.048791009932756424 , accuracy =  0.9844499826431274
ADAM: run no. =  2 , batch size =  32  convergence time =  23.557218313217163
ADAM: run no. =  2 , batch size =  32 , test loss =  0.03948014974594116 , test accuracy =  0.989799976348877
###### Running part 2: SGD with batch size  64  ######
Epoch:  1 , loss =  0.04532957449555397 , accuracy =  0.9852833151817322
Epoch:  2 , loss =  0.047198060899972916 , accuracy =  0.9853666424751282
Epoch:  3 , loss =  0.04455358162522316 , accuracy =  0.9856333136558533
Epoch:  4 , loss =  0.04539545625448227 , accuracy =  0.9852833151817322
Epoch:  5 , loss =  0.04656033590435982 , accuracy =  0.9850333333015442
Epoch:  6 , loss =  0.045480772852897644 , accuracy =  0.9853166937828064
SGD: run no. =  2 , batch size =  64  convergence time =  13.607971906661987
SGD: run no. =  2 , batch size =  64 , test loss =  0.027883509173989296 , test accuracy =  0.9907000064849854
###### Running part 2: ADAGRAD with batch size  64  ######
Epoch:  1 , loss =  0.06744115799665451 , accuracy =  0.9781500101089478
Epoch:  2 , loss =  0.06588443368673325 , accuracy =  0.9790999889373779
Epoch:  3 , loss =  0.06603981554508209 , accuracy =  0.9786166548728943
Epoch:  4 , loss =  0.06865263730287552 , accuracy =  0.9788166880607605
Epoch:  5 , loss =  0.06677243113517761 , accuracy =  0.979449987411499
ADAGRAD: run no. =  2 , batch size =  64  convergence time =  11.797988891601562
ADAGRAD: run no. =  2 , batch size =  64 , test loss =  0.038449786603450775 , test accuracy =  0.9878000020980835
###### Running part 2: ADAM with batch size  64  ######
Epoch:  1 , loss =  0.04334701970219612 , accuracy =  0.98621666431427
Epoch:  2 , loss =  0.04055747389793396 , accuracy =  0.9871000051498413
Epoch:  3 , loss =  0.0405181460082531 , accuracy =  0.9868000149726868
Epoch:  4 , loss =  0.040815334767103195 , accuracy =  0.9871000051498413
Epoch:  5 , loss =  0.04186934232711792 , accuracy =  0.9869666695594788
Epoch:  6 , loss =  0.041268762201070786 , accuracy =  0.9867166876792908
ADAM: run no. =  2 , batch size =  64  convergence time =  14.25873064994812
ADAM: run no. =  2 , batch size =  64 , test loss =  0.0360918790102005 , test accuracy =  0.989799976348877
###### Running part 2: SGD with batch size  96  ######
Epoch:  1 , loss =  0.043924566358327866 , accuracy =  0.9855499863624573
Epoch:  2 , loss =  0.04433835297822952 , accuracy =  0.9851499795913696
Epoch:  3 , loss =  0.043080009520053864 , accuracy =  0.9855166673660278
Epoch:  4 , loss =  0.04320504516363144 , accuracy =  0.9855333566665649
Epoch:  5 , loss =  0.04443812742829323 , accuracy =  0.9857666492462158
Epoch:  6 , loss =  0.043099306523799896 , accuracy =  0.985883355140686
SGD: run no. =  2 , batch size =  96  convergence time =  9.791959047317505
SGD: run no. =  2 , batch size =  96 , test loss =  0.027963299304246902 , test accuracy =  0.9902999997138977
###### Running part 2: ADAGRAD with batch size  96  ######
Epoch:  1 , loss =  0.0678318440914154 , accuracy =  0.9794333577156067
Epoch:  2 , loss =  0.06745186448097229 , accuracy =  0.9782000184059143
Epoch:  3 , loss =  0.06355886906385422 , accuracy =  0.9796666502952576
Epoch:  4 , loss =  0.06686583161354065 , accuracy =  0.9788333177566528
Epoch:  5 , loss =  0.06744442880153656 , accuracy =  0.9781500101089478
Epoch:  6 , loss =  0.0654490739107132 , accuracy =  0.9787499904632568
ADAGRAD: run no. =  2 , batch size =  96  convergence time =  10.008910417556763
ADAGRAD: run no. =  2 , batch size =  96 , test loss =  0.03722554072737694 , test accuracy =  0.9882000088691711
###### Running part 2: ADAM with batch size  96  ######
Epoch:  1 , loss =  0.04096519201993942 , accuracy =  0.9868999719619751
Epoch:  2 , loss =  0.038296982645988464 , accuracy =  0.9872833490371704
Epoch:  3 , loss =  0.0384380966424942 , accuracy =  0.987416684627533
Epoch:  4 , loss =  0.0399068146944046 , accuracy =  0.9872000217437744
Epoch:  5 , loss =  0.03929043561220169 , accuracy =  0.9870166778564453
ADAM: run no. =  2 , batch size =  96  convergence time =  8.460984468460083
ADAM: run no. =  2 , batch size =  96 , test loss =  0.03529518470168114 , test accuracy =  0.9904999732971191
###### Running part 2: SGD with batch size  128  ######
Epoch:  1 , loss =  0.042634498327970505 , accuracy =  0.9858499765396118
Epoch:  2 , loss =  0.04337606579065323 , accuracy =  0.98580002784729
Epoch:  3 , loss =  0.043683409690856934 , accuracy =  0.9859333038330078
Epoch:  4 , loss =  0.04527135565876961 , accuracy =  0.9856500029563904
SGD: run no. =  2 , batch size =  128  convergence time =  5.226446866989136
SGD: run no. =  2 , batch size =  128 , test loss =  0.027946433052420616 , test accuracy =  0.9902999997138977
###### Running part 2: ADAGRAD with batch size  128  ######
Epoch:  1 , loss =  0.0653485581278801 , accuracy =  0.9788500070571899
Epoch:  2 , loss =  0.06526751071214676 , accuracy =  0.9792166948318481
Epoch:  3 , loss =  0.06582259386777878 , accuracy =  0.9787999987602234
Epoch:  4 , loss =  0.06448478251695633 , accuracy =  0.9801999926567078
Epoch:  5 , loss =  0.06438952684402466 , accuracy =  0.9785333275794983
ADAGRAD: run no. =  2 , batch size =  128  convergence time =  6.666852951049805
ADAGRAD: run no. =  2 , batch size =  128 , test loss =  0.03681446239352226 , test accuracy =  0.9879999756813049
###### Running part 2: ADAM with batch size  128  ######
Epoch:  1 , loss =  0.03928615152835846 , accuracy =  0.9881666898727417
Epoch:  2 , loss =  0.037474703043699265 , accuracy =  0.9876333475112915
Epoch:  3 , loss =  0.037083085626363754 , accuracy =  0.9879000186920166
Epoch:  4 , loss =  0.035458024591207504 , accuracy =  0.9886000156402588
Epoch:  5 , loss =  0.03705746680498123 , accuracy =  0.9882000088691711
Epoch:  6 , loss =  0.037458669394254684 , accuracy =  0.9876333475112915
Epoch:  7 , loss =  0.03884586691856384 , accuracy =  0.9872999787330627
ADAM: run no. =  2 , batch size =  128  convergence time =  9.576838731765747
ADAM: run no. =  2 , batch size =  128 , test loss =  0.03731900081038475 , test accuracy =  0.9901999831199646
###### RUN NUMBER  3  ######
###### Running part 2: SGD with batch size  32  ######
Epoch:  1 , loss =  0.047043170779943466 , accuracy =  0.9851833581924438
Epoch:  2 , loss =  0.04767954349517822 , accuracy =  0.9841166734695435
Epoch:  3 , loss =  0.046099621802568436 , accuracy =  0.9843999743461609
Epoch:  4 , loss =  0.046105433255434036 , accuracy =  0.9843999743461609
Epoch:  5 , loss =  0.043536778539419174 , accuracy =  0.9849500060081482
Epoch:  6 , loss =  0.04572208598256111 , accuracy =  0.9853666424751282
Epoch:  7 , loss =  0.04513230919837952 , accuracy =  0.9854833483695984
Epoch:  8 , loss =  0.050326157361269 , accuracy =  0.9834499955177307
SGD: run no. =  3 , batch size =  32  convergence time =  26.491875410079956
SGD: run no. =  3 , batch size =  32 , test loss =  0.026875881478190422 , test accuracy =  0.9908999800682068
###### Running part 2: ADAGRAD with batch size  32  ######
Epoch:  1 , loss =  0.06879501044750214 , accuracy =  0.9779999852180481
Epoch:  2 , loss =  0.06790090352296829 , accuracy =  0.9784166812896729
Epoch:  3 , loss =  0.06846644729375839 , accuracy =  0.9783666729927063
Epoch:  4 , loss =  0.06938771903514862 , accuracy =  0.9778000116348267
Epoch:  5 , loss =  0.06748344749212265 , accuracy =  0.9785500168800354
Epoch:  6 , loss =  0.06640657037496567 , accuracy =  0.9788500070571899
Epoch:  7 , loss =  0.06912299245595932 , accuracy =  0.9781500101089478
Epoch:  8 , loss =  0.06647158414125443 , accuracy =  0.9793000221252441
Epoch:  9 , loss =  0.06764814257621765 , accuracy =  0.9788166880607605
ADAGRAD: run no. =  3 , batch size =  32  convergence time =  30.267523288726807
ADAGRAD: run no. =  3 , batch size =  32 , test loss =  0.03681857883930206 , test accuracy =  0.9884999990463257
###### Running part 2: ADAM with batch size  32  ######
Epoch:  1 , loss =  0.044468414038419724 , accuracy =  0.985883355140686
Epoch:  2 , loss =  0.04611169919371605 , accuracy =  0.986133337020874
Epoch:  3 , loss =  0.04626789689064026 , accuracy =  0.9854999780654907
Epoch:  4 , loss =  0.04342731460928917 , accuracy =  0.9866333603858948
Epoch:  5 , loss =  0.04589865356683731 , accuracy =  0.9854666590690613
Epoch:  6 , loss =  0.04454526677727699 , accuracy =  0.9857000112533569
Epoch:  7 , loss =  0.04566442221403122 , accuracy =  0.9857000112533569
ADAM: run no. =  3 , batch size =  32  convergence time =  23.610077142715454
ADAM: run no. =  3 , batch size =  32 , test loss =  0.03486296534538269 , test accuracy =  0.9901000261306763
###### Running part 2: SGD with batch size  64  ######
Epoch:  1 , loss =  0.04460558667778969 , accuracy =  0.9853166937828064
Epoch:  2 , loss =  0.045153841376304626 , accuracy =  0.9853500127792358
Epoch:  3 , loss =  0.042480893433094025 , accuracy =  0.986466646194458
Epoch:  4 , loss =  0.04488404095172882 , accuracy =  0.9850666522979736
Epoch:  5 , loss =  0.04283297806978226 , accuracy =  0.985966682434082
Epoch:  6 , loss =  0.04287810996174812 , accuracy =  0.9861000180244446
SGD: run no. =  3 , batch size =  64  convergence time =  13.736905574798584
SGD: run no. =  3 , batch size =  64 , test loss =  0.026944182813167572 , test accuracy =  0.9901999831199646
###### Running part 2: ADAGRAD with batch size  64  ######
Epoch:  1 , loss =  0.06298787146806717 , accuracy =  0.9801666736602783
Epoch:  2 , loss =  0.0643378347158432 , accuracy =  0.9798499941825867
Epoch:  3 , loss =  0.06214214861392975 , accuracy =  0.9801666736602783
Epoch:  4 , loss =  0.05930868908762932 , accuracy =  0.9809499979019165
Epoch:  5 , loss =  0.06263311207294464 , accuracy =  0.9801999926567078
Epoch:  6 , loss =  0.06337154656648636 , accuracy =  0.9802166819572449
Epoch:  7 , loss =  0.06135181710124016 , accuracy =  0.980400025844574
ADAGRAD: run no. =  3 , batch size =  64  convergence time =  16.355473041534424
ADAGRAD: run no. =  3 , batch size =  64 , test loss =  0.037304945290088654 , test accuracy =  0.9879000186920166
###### Running part 2: ADAM with batch size  64  ######
Epoch:  1 , loss =  0.04314097762107849 , accuracy =  0.9867166876792908
Epoch:  2 , loss =  0.041792526841163635 , accuracy =  0.9871333241462708
Epoch:  3 , loss =  0.0419035367667675 , accuracy =  0.9870833158493042
Epoch:  4 , loss =  0.04244430363178253 , accuracy =  0.986299991607666
Epoch:  5 , loss =  0.04148411378264427 , accuracy =  0.9872333407402039
Epoch:  6 , loss =  0.041587524116039276 , accuracy =  0.9866166710853577
Epoch:  7 , loss =  0.03896322473883629 , accuracy =  0.9876833558082581
Epoch:  8 , loss =  0.041971150785684586 , accuracy =  0.9868666529655457
Epoch:  9 , loss =  0.03908170387148857 , accuracy =  0.9874333143234253
Epoch:  10 , loss =  0.04124471917748451 , accuracy =  0.9868500232696533
ADAM: run no. =  3 , batch size =  64  convergence time =  23.56852149963379
ADAM: run no. =  3 , batch size =  64 , test loss =  0.03524547442793846 , test accuracy =  0.9890999794006348
###### Running part 2: SGD with batch size  96  ######
Epoch:  1 , loss =  0.04287143424153328 , accuracy =  0.9859166741371155
Epoch:  2 , loss =  0.04274008050560951 , accuracy =  0.9864166378974915
Epoch:  3 , loss =  0.04311510920524597 , accuracy =  0.9863166809082031
Epoch:  4 , loss =  0.04214616119861603 , accuracy =  0.9862833619117737
Epoch:  5 , loss =  0.041496120393276215 , accuracy =  0.9865833520889282
SGD: run no. =  3 , batch size =  96  convergence time =  8.094677686691284
SGD: run no. =  3 , batch size =  96 , test loss =  0.027304230257868767 , test accuracy =  0.9909999966621399
###### Running part 2: ADAGRAD with batch size  96  ######
Epoch:  1 , loss =  0.06766826659440994 , accuracy =  0.9785666465759277
Epoch:  2 , loss =  0.06097543239593506 , accuracy =  0.9807833433151245
Epoch:  3 , loss =  0.06293857842683792 , accuracy =  0.9798833131790161
Epoch:  4 , loss =  0.06468728929758072 , accuracy =  0.9804333448410034
Epoch:  5 , loss =  0.06184849143028259 , accuracy =  0.9802666902542114
ADAGRAD: run no. =  3 , batch size =  96  convergence time =  8.261723279953003
ADAGRAD: run no. =  3 , batch size =  96 , test loss =  0.036859847605228424 , test accuracy =  0.9879999756813049
###### Running part 2: ADAM with batch size  96  ######
Epoch:  1 , loss =  0.04029553756117821 , accuracy =  0.9868833422660828
Epoch:  2 , loss =  0.03663153201341629 , accuracy =  0.988183319568634
Epoch:  3 , loss =  0.037744514644145966 , accuracy =  0.9879500269889832
Epoch:  4 , loss =  0.036594901233911514 , accuracy =  0.9883166551589966
Epoch:  5 , loss =  0.03618593513965607 , accuracy =  0.9880333542823792
Epoch:  6 , loss =  0.03890348598361015 , accuracy =  0.987500011920929
Epoch:  7 , loss =  0.03878035768866539 , accuracy =  0.9874333143234253
Epoch:  8 , loss =  0.037413228303194046 , accuracy =  0.9880666732788086
ADAM: run no. =  3 , batch size =  96  convergence time =  13.337709426879883
ADAM: run no. =  3 , batch size =  96 , test loss =  0.03598221018910408 , test accuracy =  0.9897000193595886
###### Running part 2: SGD with batch size  128  ######
Epoch:  1 , loss =  0.04096979647874832 , accuracy =  0.9860833287239075
Epoch:  2 , loss =  0.040629707276821136 , accuracy =  0.9866499900817871
Epoch:  3 , loss =  0.039691805839538574 , accuracy =  0.9872166514396667
Epoch:  4 , loss =  0.040526386350393295 , accuracy =  0.9864833354949951
Epoch:  5 , loss =  0.04076945409178734 , accuracy =  0.9870833158493042
Epoch:  6 , loss =  0.040962181985378265 , accuracy =  0.9868833422660828
SGD: run no. =  3 , batch size =  128  convergence time =  7.870705604553223
SGD: run no. =  3 , batch size =  128 , test loss =  0.026615243405103683 , test accuracy =  0.9912999868392944
###### Running part 2: ADAGRAD with batch size  128  ######
Epoch:  1 , loss =  0.06332302838563919 , accuracy =  0.9799666404724121
Epoch:  2 , loss =  0.05963461101055145 , accuracy =  0.980566680431366
Epoch:  3 , loss =  0.062135063111782074 , accuracy =  0.9796666502952576
Epoch:  4 , loss =  0.061136823147535324 , accuracy =  0.9808666706085205
Epoch:  5 , loss =  0.06208864971995354 , accuracy =  0.9798333048820496
ADAGRAD: run no. =  3 , batch size =  128  convergence time =  6.58315634727478
ADAGRAD: run no. =  3 , batch size =  128 , test loss =  0.037512198090553284 , test accuracy =  0.9879999756813049
###### Running part 2: ADAM with batch size  128  ######
Epoch:  1 , loss =  0.036690160632133484 , accuracy =  0.9884833097457886
Epoch:  2 , loss =  0.0350310355424881 , accuracy =  0.9886666536331177
Epoch:  3 , loss =  0.03761334717273712 , accuracy =  0.9879999756813049
Epoch:  4 , loss =  0.03602141886949539 , accuracy =  0.988099992275238
Epoch:  5 , loss =  0.034785497933626175 , accuracy =  0.9887999892234802
Epoch:  6 , loss =  0.03402294963598251 , accuracy =  0.9890999794006348
Epoch:  7 , loss =  0.035482194274663925 , accuracy =  0.9891166687011719
Epoch:  8 , loss =  0.03556316718459129 , accuracy =  0.9885666370391846
Epoch:  9 , loss =  0.03472070023417473 , accuracy =  0.9887833595275879
ADAM: run no. =  3 , batch size =  128  convergence time =  12.036949157714844
ADAM: run no. =  3 , batch size =  128 , test loss =  0.03482215106487274 , test accuracy =  0.9905999898910522
###### RUN NUMBER  4  ######
###### Running part 2: SGD with batch size  32  ######
Epoch:  1 , loss =  0.04448573663830757 , accuracy =  0.9853666424751282
Epoch:  2 , loss =  0.044286519289016724 , accuracy =  0.985450029373169
Epoch:  3 , loss =  0.04493221268057823 , accuracy =  0.9851166605949402
Epoch:  4 , loss =  0.04465218260884285 , accuracy =  0.9849833250045776
Epoch:  5 , loss =  0.044355787336826324 , accuracy =  0.9854833483695984
SGD: run no. =  4 , batch size =  32  convergence time =  15.974920511245728
SGD: run no. =  4 , batch size =  32 , test loss =  0.027089141309261322 , test accuracy =  0.9905999898910522
###### Running part 2: ADAGRAD with batch size  32  ######
Epoch:  1 , loss =  0.06331345438957214 , accuracy =  0.9802666902542114
Epoch:  2 , loss =  0.06526917219161987 , accuracy =  0.979200005531311
Epoch:  3 , loss =  0.0621277280151844 , accuracy =  0.9800833463668823
Epoch:  4 , loss =  0.06307604163885117 , accuracy =  0.9807000160217285
Epoch:  5 , loss =  0.06481800228357315 , accuracy =  0.9798166751861572
Epoch:  6 , loss =  0.06505853682756424 , accuracy =  0.9797499775886536
ADAGRAD: run no. =  4 , batch size =  32  convergence time =  19.786325454711914
ADAGRAD: run no. =  4 , batch size =  32 , test loss =  0.037363894283771515 , test accuracy =  0.988099992275238
###### Running part 2: ADAM with batch size  32  ######
Epoch:  1 , loss =  0.045699238777160645 , accuracy =  0.9857166409492493
Epoch:  2 , loss =  0.04402145743370056 , accuracy =  0.98621666431427
Epoch:  3 , loss =  0.043896619230508804 , accuracy =  0.9858333468437195
Epoch:  4 , loss =  0.04050211235880852 , accuracy =  0.9869666695594788
Epoch:  5 , loss =  0.041620682924985886 , accuracy =  0.9863333106040955
Epoch:  6 , loss =  0.04329611361026764 , accuracy =  0.986133337020874
Epoch:  7 , loss =  0.044410835951566696 , accuracy =  0.985966682434082
ADAM: run no. =  4 , batch size =  32  convergence time =  23.218672037124634
ADAM: run no. =  4 , batch size =  32 , test loss =  0.04169517382979393 , test accuracy =  0.9890000224113464
###### Running part 2: SGD with batch size  64  ######
Epoch:  1 , loss =  0.0421924851834774 , accuracy =  0.986133337020874
Epoch:  2 , loss =  0.041434966027736664 , accuracy =  0.9865166544914246
Epoch:  3 , loss =  0.039698608219623566 , accuracy =  0.9871666431427002
Epoch:  4 , loss =  0.04333563148975372 , accuracy =  0.9859499931335449
Epoch:  5 , loss =  0.042899128049612045 , accuracy =  0.9855666756629944
Epoch:  6 , loss =  0.04214562103152275 , accuracy =  0.9866666793823242
SGD: run no. =  4 , batch size =  64  convergence time =  13.460254907608032
SGD: run no. =  4 , batch size =  64 , test loss =  0.02665257640182972 , test accuracy =  0.991100013256073
###### Running part 2: ADAGRAD with batch size  64  ######
Epoch:  1 , loss =  0.06358731538057327 , accuracy =  0.9797833561897278
Epoch:  2 , loss =  0.06071688234806061 , accuracy =  0.9805499911308289
Epoch:  3 , loss =  0.061980586498975754 , accuracy =  0.980400025844574
Epoch:  4 , loss =  0.06082942709326744 , accuracy =  0.980400025844574
Epoch:  5 , loss =  0.063148632645607 , accuracy =  0.9793833494186401
ADAGRAD: run no. =  4 , batch size =  64  convergence time =  11.463069200515747
ADAGRAD: run no. =  4 , batch size =  64 , test loss =  0.03647502139210701 , test accuracy =  0.987500011920929
###### Running part 2: ADAM with batch size  64  ######
Epoch:  1 , loss =  0.039757631719112396 , accuracy =  0.9870499968528748
Epoch:  2 , loss =  0.038787759840488434 , accuracy =  0.9878333210945129
Epoch:  3 , loss =  0.03749186918139458 , accuracy =  0.9878666400909424
Epoch:  4 , loss =  0.0384749099612236 , accuracy =  0.9878833293914795
Epoch:  5 , loss =  0.03707411140203476 , accuracy =  0.9887333512306213
Epoch:  6 , loss =  0.039248477667570114 , accuracy =  0.9878000020980835
Epoch:  7 , loss =  0.03757874295115471 , accuracy =  0.9881666898727417
Epoch:  8 , loss =  0.03783717751502991 , accuracy =  0.9879166483879089
ADAM: run no. =  4 , batch size =  64  convergence time =  18.68230628967285
ADAM: run no. =  4 , batch size =  64 , test loss =  0.0372038371860981 , test accuracy =  0.989799976348877
###### Running part 2: SGD with batch size  96  ######
Epoch:  1 , loss =  0.03971441090106964 , accuracy =  0.9865666627883911
Epoch:  2 , loss =  0.040660273283720016 , accuracy =  0.9867500066757202
Epoch:  3 , loss =  0.04048768803477287 , accuracy =  0.986466646194458
Epoch:  4 , loss =  0.041151951998472214 , accuracy =  0.9867833256721497
SGD: run no. =  4 , batch size =  96  convergence time =  6.4747538566589355
SGD: run no. =  4 , batch size =  96 , test loss =  0.026690514758229256 , test accuracy =  0.9908000230789185
###### Running part 2: ADAGRAD with batch size  96  ######
Epoch:  1 , loss =  0.06251724809408188 , accuracy =  0.9798833131790161
Epoch:  2 , loss =  0.06052912026643753 , accuracy =  0.9806333184242249
Epoch:  3 , loss =  0.06183728575706482 , accuracy =  0.9798333048820496
Epoch:  4 , loss =  0.061386238783597946 , accuracy =  0.9800999760627747
Epoch:  5 , loss =  0.06149011477828026 , accuracy =  0.9808666706085205
ADAGRAD: run no. =  4 , batch size =  96  convergence time =  8.492154121398926
ADAGRAD: run no. =  4 , batch size =  96 , test loss =  0.03619813174009323 , test accuracy =  0.9876999855041504
###### Running part 2: ADAM with batch size  96  ######
Epoch:  1 , loss =  0.036559514701366425 , accuracy =  0.9883666634559631
Epoch:  2 , loss =  0.03522910177707672 , accuracy =  0.9891166687011719
Epoch:  3 , loss =  0.0355692058801651 , accuracy =  0.988183319568634
Epoch:  4 , loss =  0.03324216976761818 , accuracy =  0.9893666505813599
Epoch:  5 , loss =  0.03571385145187378 , accuracy =  0.9886999726295471
Epoch:  6 , loss =  0.034540437161922455 , accuracy =  0.9896000027656555
Epoch:  7 , loss =  0.03512636572122574 , accuracy =  0.9891666769981384
ADAM: run no. =  4 , batch size =  96  convergence time =  11.950318813323975
ADAM: run no. =  4 , batch size =  96 , test loss =  0.038491301238536835 , test accuracy =  0.9905999898910522
###### Running part 2: SGD with batch size  128  ######
Epoch:  1 , loss =  0.040373388677835464 , accuracy =  0.9867666959762573
Epoch:  2 , loss =  0.03894650191068649 , accuracy =  0.987333357334137
Epoch:  3 , loss =  0.04072659835219383 , accuracy =  0.9863666892051697
Epoch:  4 , loss =  0.038551878184080124 , accuracy =  0.9872666597366333
Epoch:  5 , loss =  0.04041312262415886 , accuracy =  0.9866833090782166
Epoch:  6 , loss =  0.03988872095942497 , accuracy =  0.9871000051498413
Epoch:  7 , loss =  0.03811486065387726 , accuracy =  0.9873166680335999
Epoch:  8 , loss =  0.04071786627173424 , accuracy =  0.9865333437919617
Epoch:  9 , loss =  0.0403679795563221 , accuracy =  0.9867500066757202
Epoch:  10 , loss =  0.040190938860177994 , accuracy =  0.9865999817848206
SGD: run no. =  4 , batch size =  128  convergence time =  12.925641298294067
SGD: run no. =  4 , batch size =  128 , test loss =  0.027309900149703026 , test accuracy =  0.9909999966621399
###### Running part 2: ADAGRAD with batch size  128  ######
Epoch:  1 , loss =  0.061030518263578415 , accuracy =  0.9799833297729492
Epoch:  2 , loss =  0.06243569031357765 , accuracy =  0.9802333116531372
Epoch:  3 , loss =  0.05827602371573448 , accuracy =  0.981333315372467
Epoch:  4 , loss =  0.060268569737672806 , accuracy =  0.9803166389465332
Epoch:  5 , loss =  0.06104905158281326 , accuracy =  0.9809166789054871
Epoch:  6 , loss =  0.060460563749074936 , accuracy =  0.980733335018158
ADAGRAD: run no. =  4 , batch size =  128  convergence time =  7.877198696136475
ADAGRAD: run no. =  4 , batch size =  128 , test loss =  0.03636215999722481 , test accuracy =  0.9879000186920166
###### Running part 2: ADAM with batch size  128  ######
Epoch:  1 , loss =  0.033966559916734695 , accuracy =  0.9893500208854675
Epoch:  2 , loss =  0.034707508981227875 , accuracy =  0.9888666868209839
Epoch:  3 , loss =  0.035615455359220505 , accuracy =  0.9888666868209839
Epoch:  4 , loss =  0.0330074168741703 , accuracy =  0.989633321762085
Epoch:  5 , loss =  0.03347148001194 , accuracy =  0.9890000224113464
Epoch:  6 , loss =  0.03315810114145279 , accuracy =  0.9897666573524475
Epoch:  7 , loss =  0.032897330820560455 , accuracy =  0.989633321762085
Epoch:  8 , loss =  0.03270686790347099 , accuracy =  0.9893333315849304
ADAM: run no. =  4 , batch size =  128  convergence time =  10.646150350570679
ADAM: run no. =  4 , batch size =  128 , test loss =  0.03759132698178291 , test accuracy =  0.9898999929428101
###### RUN NUMBER  5  ######
###### Running part 2: SGD with batch size  32  ######
Epoch:  1 , loss =  0.042116835713386536 , accuracy =  0.98621666431427
Epoch:  2 , loss =  0.04494526609778404 , accuracy =  0.9851999878883362
Epoch:  3 , loss =  0.045421674847602844 , accuracy =  0.9849833250045776
Epoch:  4 , loss =  0.042539265006780624 , accuracy =  0.985450029373169
SGD: run no. =  5 , batch size =  32  convergence time =  12.579176187515259
SGD: run no. =  5 , batch size =  32 , test loss =  0.027535712346434593 , test accuracy =  0.9901000261306763
###### Running part 2: ADAGRAD with batch size  32  ######
Epoch:  1 , loss =  0.06078963726758957 , accuracy =  0.980816662311554
Epoch:  2 , loss =  0.0610787533223629 , accuracy =  0.9805166721343994
Epoch:  3 , loss =  0.0627317726612091 , accuracy =  0.9799000024795532
Epoch:  4 , loss =  0.06150953471660614 , accuracy =  0.9796833395957947
ADAGRAD: run no. =  5 , batch size =  32  convergence time =  13.136136531829834
ADAGRAD: run no. =  5 , batch size =  32 , test loss =  0.036528825759887695 , test accuracy =  0.9876000285148621
###### Running part 2: ADAM with batch size  32  ######
Epoch:  1 , loss =  0.040677640587091446 , accuracy =  0.9872333407402039
Epoch:  2 , loss =  0.04078259319067001 , accuracy =  0.9871333241462708
Epoch:  3 , loss =  0.03884994238615036 , accuracy =  0.9880666732788086
Epoch:  4 , loss =  0.04011962190270424 , accuracy =  0.9877166748046875
Epoch:  5 , loss =  0.04229394346475601 , accuracy =  0.9866666793823242
Epoch:  6 , loss =  0.039797160774469376 , accuracy =  0.9879833459854126
ADAM: run no. =  5 , batch size =  32  convergence time =  19.7932448387146
ADAM: run no. =  5 , batch size =  32 , test loss =  0.03525719419121742 , test accuracy =  0.9908000230789185
###### Running part 2: SGD with batch size  64  ######
Epoch:  1 , loss =  0.04312600567936897 , accuracy =  0.9857833385467529
Epoch:  2 , loss =  0.04070934280753136 , accuracy =  0.9866999983787537
Epoch:  3 , loss =  0.040063709020614624 , accuracy =  0.9868500232696533
Epoch:  4 , loss =  0.04216215759515762 , accuracy =  0.9865833520889282
Epoch:  5 , loss =  0.04004201665520668 , accuracy =  0.9865666627883911
Epoch:  6 , loss =  0.04185406491160393 , accuracy =  0.9862666726112366
Epoch:  7 , loss =  0.03981490805745125 , accuracy =  0.9868000149726868
SGD: run no. =  5 , batch size =  64  convergence time =  15.587158441543579
SGD: run no. =  5 , batch size =  64 , test loss =  0.026323867961764336 , test accuracy =  0.9909999966621399
###### Running part 2: ADAGRAD with batch size  64  ######
Epoch:  1 , loss =  0.061256106942892075 , accuracy =  0.9799166917800903
Epoch:  2 , loss =  0.06192214787006378 , accuracy =  0.9798666834831238
Epoch:  3 , loss =  0.062003109604120255 , accuracy =  0.9810000061988831
Epoch:  4 , loss =  0.06263242661952972 , accuracy =  0.98048335313797
ADAGRAD: run no. =  5 , batch size =  64  convergence time =  9.124576091766357
ADAGRAD: run no. =  5 , batch size =  64 , test loss =  0.03671380877494812 , test accuracy =  0.9886999726295471
###### Running part 2: ADAM with batch size  64  ######
Epoch:  1 , loss =  0.03833690658211708 , accuracy =  0.9879166483879089
Epoch:  2 , loss =  0.03611278906464577 , accuracy =  0.9885500073432922
Epoch:  3 , loss =  0.03506778925657272 , accuracy =  0.9883666634559631
Epoch:  4 , loss =  0.03642123192548752 , accuracy =  0.9880666732788086
Epoch:  5 , loss =  0.03613883629441261 , accuracy =  0.9880833625793457
Epoch:  6 , loss =  0.037119779735803604 , accuracy =  0.9878833293914795
ADAM: run no. =  5 , batch size =  64  convergence time =  13.95463490486145
ADAM: run no. =  5 , batch size =  64 , test loss =  0.03984054923057556 , test accuracy =  0.989300012588501
###### Running part 2: SGD with batch size  96  ######
Epoch:  1 , loss =  0.037117183208465576 , accuracy =  0.9876333475112915
Epoch:  2 , loss =  0.03938376531004906 , accuracy =  0.9870166778564453
Epoch:  3 , loss =  0.03678227588534355 , accuracy =  0.9873666763305664
Epoch:  4 , loss =  0.03897600620985031 , accuracy =  0.9868500232696533
Epoch:  5 , loss =  0.039935678243637085 , accuracy =  0.9868999719619751
Epoch:  6 , loss =  0.0375669002532959 , accuracy =  0.9874333143234253
SGD: run no. =  5 , batch size =  96  convergence time =  9.592681646347046
SGD: run no. =  5 , batch size =  96 , test loss =  0.027037885040044785 , test accuracy =  0.9907000064849854
###### Running part 2: ADAGRAD with batch size  96  ######
Epoch:  1 , loss =  0.0595085471868515 , accuracy =  0.9801499843597412
Epoch:  2 , loss =  0.058849118649959564 , accuracy =  0.98089998960495
Epoch:  3 , loss =  0.060620855540037155 , accuracy =  0.981083333492279
Epoch:  4 , loss =  0.0595930851995945 , accuracy =  0.9808333516120911
Epoch:  5 , loss =  0.05871094390749931 , accuracy =  0.980983316898346
Epoch:  6 , loss =  0.058317769318819046 , accuracy =  0.9816166758537292
ADAGRAD: run no. =  5 , batch size =  96  convergence time =  9.814542293548584
ADAGRAD: run no. =  5 , batch size =  96 , test loss =  0.03661126643419266 , test accuracy =  0.9878000020980835
###### Running part 2: ADAM with batch size  96  ######
Epoch:  1 , loss =  0.03402797132730484 , accuracy =  0.989300012588501
Epoch:  2 , loss =  0.03395908698439598 , accuracy =  0.9888166785240173
Epoch:  3 , loss =  0.034709591418504715 , accuracy =  0.9885166883468628
Epoch:  4 , loss =  0.036151085048913956 , accuracy =  0.98826664686203
Epoch:  5 , loss =  0.0332740880548954 , accuracy =  0.9897500276565552
Epoch:  6 , loss =  0.03485222905874252 , accuracy =  0.9893500208854675
Epoch:  7 , loss =  0.033773500472307205 , accuracy =  0.9894999861717224
Epoch:  8 , loss =  0.03403737023472786 , accuracy =  0.9892333149909973
ADAM: run no. =  5 , batch size =  96  convergence time =  13.222237348556519
ADAM: run no. =  5 , batch size =  96 , test loss =  0.03951378911733627 , test accuracy =  0.9902999997138977
###### Running part 2: SGD with batch size  128  ######
Epoch:  1 , loss =  0.03812893480062485 , accuracy =  0.9872999787330627
Epoch:  2 , loss =  0.039313822984695435 , accuracy =  0.9873833060264587
Epoch:  3 , loss =  0.039288848638534546 , accuracy =  0.9876166582107544
Epoch:  4 , loss =  0.03987797349691391 , accuracy =  0.9870166778564453
SGD: run no. =  5 , batch size =  128  convergence time =  5.171683073043823
SGD: run no. =  5 , batch size =  128 , test loss =  0.026792675256729126 , test accuracy =  0.9912999868392944
###### Running part 2: ADAGRAD with batch size  128  ######
Epoch:  1 , loss =  0.06224711611866951 , accuracy =  0.9802500009536743
Epoch:  2 , loss =  0.06011192500591278 , accuracy =  0.981083333492279
Epoch:  3 , loss =  0.059815872460603714 , accuracy =  0.981166660785675
Epoch:  4 , loss =  0.05828293412923813 , accuracy =  0.9823333621025085
Epoch:  5 , loss =  0.060680218040943146 , accuracy =  0.9810333251953125
Epoch:  6 , loss =  0.058247048407793045 , accuracy =  0.9807500243186951
Epoch:  7 , loss =  0.058605607599020004 , accuracy =  0.9814666509628296
Epoch:  8 , loss =  0.05709349736571312 , accuracy =  0.9815000295639038
Epoch:  9 , loss =  0.05904700607061386 , accuracy =  0.9816333055496216
Epoch:  10 , loss =  0.05943404510617256 , accuracy =  0.9813166856765747
Epoch:  11 , loss =  0.059122998267412186 , accuracy =  0.9811833500862122
ADAGRAD: run no. =  5 , batch size =  128  convergence time =  14.426962614059448
ADAGRAD: run no. =  5 , batch size =  128 , test loss =  0.03707059100270271 , test accuracy =  0.9879000186920166
###### Running part 2: ADAM with batch size  128  ######
Epoch:  1 , loss =  0.03373037651181221 , accuracy =  0.9888333082199097
Epoch:  2 , loss =  0.03494015708565712 , accuracy =  0.9889166951179504
Epoch:  3 , loss =  0.03395450487732887 , accuracy =  0.9890000224113464
Epoch:  4 , loss =  0.03356734663248062 , accuracy =  0.989799976348877
Epoch:  5 , loss =  0.03227191045880318 , accuracy =  0.9901000261306763
Epoch:  6 , loss =  0.03337225690484047 , accuracy =  0.9891666769981384
Epoch:  7 , loss =  0.03373754024505615 , accuracy =  0.9892333149909973
Epoch:  8 , loss =  0.03369862958788872 , accuracy =  0.9895333051681519
ADAM: run no. =  5 , batch size =  128  convergence time =  10.571191310882568
ADAM: run no. =  5 , batch size =  128 , test loss =  0.035031240433454514 , test accuracy =  0.9901000261306763
###### RUN NUMBER  6  ######
###### Running part 2: SGD with batch size  32  ######
Epoch:  1 , loss =  0.0421210415661335 , accuracy =  0.9862666726112366
Epoch:  2 , loss =  0.042119987308979034 , accuracy =  0.9864833354949951
Epoch:  3 , loss =  0.0421527698636055 , accuracy =  0.9858999848365784
Epoch:  4 , loss =  0.041015878319740295 , accuracy =  0.9862666726112366
Epoch:  5 , loss =  0.04363156110048294 , accuracy =  0.98580002784729
Epoch:  6 , loss =  0.041715189814567566 , accuracy =  0.9865666627883911
Epoch:  7 , loss =  0.041286736726760864 , accuracy =  0.9870166778564453
SGD: run no. =  6 , batch size =  32  convergence time =  22.05966091156006
SGD: run no. =  6 , batch size =  32 , test loss =  0.028446398675441742 , test accuracy =  0.9909999966621399
###### Running part 2: ADAGRAD with batch size  32  ######
Epoch:  1 , loss =  0.057033032178878784 , accuracy =  0.9818166494369507
Epoch:  2 , loss =  0.06211472675204277 , accuracy =  0.9801166653633118
Epoch:  3 , loss =  0.06307164579629898 , accuracy =  0.9801166653633118
Epoch:  4 , loss =  0.059498149901628494 , accuracy =  0.9814333319664001
ADAGRAD: run no. =  6 , batch size =  32  convergence time =  13.049041748046875
ADAGRAD: run no. =  6 , batch size =  32 , test loss =  0.03669094294309616 , test accuracy =  0.9876000285148621
###### Running part 2: ADAM with batch size  32  ######
Epoch:  1 , loss =  0.03943140059709549 , accuracy =  0.9872666597366333
Epoch:  2 , loss =  0.03820541873574257 , accuracy =  0.9876166582107544
Epoch:  3 , loss =  0.04098252207040787 , accuracy =  0.9876499772071838
Epoch:  4 , loss =  0.04208541661500931 , accuracy =  0.9869999885559082
Epoch:  5 , loss =  0.039577070623636246 , accuracy =  0.9875166416168213
ADAM: run no. =  6 , batch size =  32  convergence time =  16.343416452407837
ADAM: run no. =  6 , batch size =  32 , test loss =  0.039342716336250305 , test accuracy =  0.9894999861717224
###### Running part 2: SGD with batch size  64  ######
Epoch:  1 , loss =  0.038701675832271576 , accuracy =  0.9872666597366333
Epoch:  2 , loss =  0.038784418255090714 , accuracy =  0.9872000217437744
Epoch:  3 , loss =  0.037681810557842255 , accuracy =  0.9875666499137878
Epoch:  4 , loss =  0.03783268481492996 , accuracy =  0.9871000051498413
Epoch:  5 , loss =  0.038925204426050186 , accuracy =  0.9873666763305664
Epoch:  6 , loss =  0.03735196217894554 , accuracy =  0.9877166748046875
Epoch:  7 , loss =  0.03833099827170372 , accuracy =  0.9875166416168213
Epoch:  8 , loss =  0.03773757442831993 , accuracy =  0.9876833558082581
Epoch:  9 , loss =  0.036756958812475204 , accuracy =  0.988349974155426
Epoch:  10 , loss =  0.038926027715206146 , accuracy =  0.9874833226203918
Epoch:  11 , loss =  0.03681032359600067 , accuracy =  0.9879166483879089
Epoch:  12 , loss =  0.038456764072179794 , accuracy =  0.9873999953269958
SGD: run no. =  6 , batch size =  64  convergence time =  26.387574672698975
SGD: run no. =  6 , batch size =  64 , test loss =  0.02718387171626091 , test accuracy =  0.9909999966621399
###### Running part 2: ADAGRAD with batch size  64  ######
Epoch:  1 , loss =  0.06046806648373604 , accuracy =  0.98089998960495
Epoch:  2 , loss =  0.06094752252101898 , accuracy =  0.9803833365440369
Epoch:  3 , loss =  0.05912952870130539 , accuracy =  0.980983316898346
Epoch:  4 , loss =  0.05836371332406998 , accuracy =  0.9815000295639038
Epoch:  5 , loss =  0.06066190078854561 , accuracy =  0.9813500046730042
Epoch:  6 , loss =  0.05991923436522484 , accuracy =  0.9806833267211914
Epoch:  7 , loss =  0.059746067970991135 , accuracy =  0.9809333086013794
ADAGRAD: run no. =  6 , batch size =  64  convergence time =  16.071216583251953
ADAGRAD: run no. =  6 , batch size =  64 , test loss =  0.03634876012802124 , test accuracy =  0.9886000156402588
###### Running part 2: ADAM with batch size  64  ######
Epoch:  1 , loss =  0.03823896497488022 , accuracy =  0.9878000020980835
Epoch:  2 , loss =  0.03746594488620758 , accuracy =  0.98826664686203
Epoch:  3 , loss =  0.035087063908576965 , accuracy =  0.9892833232879639
Epoch:  4 , loss =  0.03520580381155014 , accuracy =  0.9888499975204468
Epoch:  5 , loss =  0.034722913056612015 , accuracy =  0.9884333610534668
Epoch:  6 , loss =  0.034735217690467834 , accuracy =  0.9884833097457886
Epoch:  7 , loss =  0.037001144140958786 , accuracy =  0.9883000254631042
Epoch:  8 , loss =  0.033770665526390076 , accuracy =  0.9889000058174133
Epoch:  9 , loss =  0.036436472088098526 , accuracy =  0.9886000156402588
Epoch:  10 , loss =  0.03454044461250305 , accuracy =  0.9890499711036682
Epoch:  11 , loss =  0.033824872225522995 , accuracy =  0.9891166687011719
ADAM: run no. =  6 , batch size =  64  convergence time =  26.048827648162842
ADAM: run no. =  6 , batch size =  64 , test loss =  0.034768134355545044 , test accuracy =  0.9904999732971191
###### Running part 2: SGD with batch size  96  ######
Epoch:  1 , loss =  0.0382273867726326 , accuracy =  0.9877833127975464
Epoch:  2 , loss =  0.036156609654426575 , accuracy =  0.9877833127975464
Epoch:  3 , loss =  0.0362996980547905 , accuracy =  0.9880833625793457
Epoch:  4 , loss =  0.03771815076470375 , accuracy =  0.9872499704360962
Epoch:  5 , loss =  0.037579622119665146 , accuracy =  0.988016664981842
SGD: run no. =  6 , batch size =  96  convergence time =  7.948148965835571
SGD: run no. =  6 , batch size =  96 , test loss =  0.026583947241306305 , test accuracy =  0.9912999868392944
###### Running part 2: ADAGRAD with batch size  96  ######
Epoch:  1 , loss =  0.0595233254134655 , accuracy =  0.9809666872024536
Epoch:  2 , loss =  0.05908086895942688 , accuracy =  0.9819166660308838
Epoch:  3 , loss =  0.05863499641418457 , accuracy =  0.9809666872024536
Epoch:  4 , loss =  0.057873792946338654 , accuracy =  0.9819333553314209
ADAGRAD: run no. =  6 , batch size =  96  convergence time =  6.635963439941406
ADAGRAD: run no. =  6 , batch size =  96 , test loss =  0.03737388923764229 , test accuracy =  0.9879000186920166
###### Running part 2: ADAM with batch size  96  ######
Epoch:  1 , loss =  0.03344787657260895 , accuracy =  0.9889666438102722
Epoch:  2 , loss =  0.03185735270380974 , accuracy =  0.9896833300590515
Epoch:  3 , loss =  0.03312133252620697 , accuracy =  0.9894333481788635
Epoch:  4 , loss =  0.03423150256276131 , accuracy =  0.989216685295105
Epoch:  5 , loss =  0.03270498663187027 , accuracy =  0.9898999929428101
ADAM: run no. =  6 , batch size =  96  convergence time =  8.224731683731079
ADAM: run no. =  6 , batch size =  96 , test loss =  0.036765340715646744 , test accuracy =  0.9904000163078308
###### Running part 2: SGD with batch size  128  ######
Epoch:  1 , loss =  0.03565152361989021 , accuracy =  0.988183319568634
Epoch:  2 , loss =  0.03712718188762665 , accuracy =  0.987933337688446
Epoch:  3 , loss =  0.03615644574165344 , accuracy =  0.9879666566848755
Epoch:  4 , loss =  0.03637020289897919 , accuracy =  0.987666666507721
SGD: run no. =  6 , batch size =  128  convergence time =  5.1337573528289795
SGD: run no. =  6 , batch size =  128 , test loss =  0.026911752298474312 , test accuracy =  0.991100013256073
###### Running part 2: ADAGRAD with batch size  128  ######
Epoch:  1 , loss =  0.05647166818380356 , accuracy =  0.9817833304405212
Epoch:  2 , loss =  0.059001535177230835 , accuracy =  0.9813166856765747
Epoch:  3 , loss =  0.05747506394982338 , accuracy =  0.9813666939735413
Epoch:  4 , loss =  0.058230482041835785 , accuracy =  0.9818333387374878
ADAGRAD: run no. =  6 , batch size =  128  convergence time =  5.2083094120025635
ADAGRAD: run no. =  6 , batch size =  128 , test loss =  0.0370941236615181 , test accuracy =  0.988099992275238
###### Running part 2: ADAM with batch size  128  ######
Epoch:  1 , loss =  0.031026050448417664 , accuracy =  0.9900833368301392
Epoch:  2 , loss =  0.03048359975218773 , accuracy =  0.9896000027656555
Epoch:  3 , loss =  0.03034643642604351 , accuracy =  0.9904833436012268
Epoch:  4 , loss =  0.031507909297943115 , accuracy =  0.9900333285331726
Epoch:  5 , loss =  0.03166940063238144 , accuracy =  0.9898499846458435
Epoch:  6 , loss =  0.033506110310554504 , accuracy =  0.9892333149909973
ADAM: run no. =  6 , batch size =  128  convergence time =  7.915226459503174
ADAM: run no. =  6 , batch size =  128 , test loss =  0.03603826090693474 , test accuracy =  0.9908000230789185
###### RUN NUMBER  7  ######
###### Running part 2: SGD with batch size  32  ######
Epoch:  1 , loss =  0.038430072367191315 , accuracy =  0.9874500036239624
Epoch:  2 , loss =  0.03888554871082306 , accuracy =  0.986549973487854
Epoch:  3 , loss =  0.03975920006632805 , accuracy =  0.9871166944503784
Epoch:  4 , loss =  0.039854779839515686 , accuracy =  0.9868166446685791
SGD: run no. =  7 , batch size =  32  convergence time =  12.37460970878601
SGD: run no. =  7 , batch size =  32 , test loss =  0.02761082723736763 , test accuracy =  0.9911999702453613
###### Running part 2: ADAGRAD with batch size  32  ######
Epoch:  1 , loss =  0.06020994856953621 , accuracy =  0.980566680431366
Epoch:  2 , loss =  0.060028765350580215 , accuracy =  0.9806333184242249
Epoch:  3 , loss =  0.059064775705337524 , accuracy =  0.98048335313797
Epoch:  4 , loss =  0.059362705796957016 , accuracy =  0.9814500212669373
Epoch:  5 , loss =  0.05824359878897667 , accuracy =  0.9808333516120911
ADAGRAD: run no. =  7 , batch size =  32  convergence time =  16.046496868133545
ADAGRAD: run no. =  7 , batch size =  32 , test loss =  0.036718953400850296 , test accuracy =  0.9884999990463257
###### Running part 2: ADAM with batch size  32  ######
Epoch:  1 , loss =  0.03938187286257744 , accuracy =  0.9877333045005798
Epoch:  2 , loss =  0.03746875002980232 , accuracy =  0.9883166551589966
Epoch:  3 , loss =  0.03642166778445244 , accuracy =  0.9882333278656006
Epoch:  4 , loss =  0.03884105756878853 , accuracy =  0.9875666499137878
Epoch:  5 , loss =  0.037888966500759125 , accuracy =  0.9875333309173584
Epoch:  6 , loss =  0.040566056966781616 , accuracy =  0.9876000285148621
ADAM: run no. =  7 , batch size =  32  convergence time =  19.427106857299805
ADAM: run no. =  7 , batch size =  32 , test loss =  0.043050508946180344 , test accuracy =  0.9896000027656555
###### Running part 2: SGD with batch size  64  ######
Epoch:  1 , loss =  0.03703930228948593 , accuracy =  0.9874833226203918
Epoch:  2 , loss =  0.038386955857276917 , accuracy =  0.9875666499137878
Epoch:  3 , loss =  0.03749990090727806 , accuracy =  0.9871833324432373
Epoch:  4 , loss =  0.03734824061393738 , accuracy =  0.9879000186920166
SGD: run no. =  7 , batch size =  64  convergence time =  8.825194358825684
SGD: run no. =  7 , batch size =  64 , test loss =  0.027107147499918938 , test accuracy =  0.9912999868392944
###### Running part 2: ADAGRAD with batch size  64  ######
Epoch:  1 , loss =  0.05887272581458092 , accuracy =  0.98089998960495
Epoch:  2 , loss =  0.05730143561959267 , accuracy =  0.9817166924476624
Epoch:  3 , loss =  0.06015283986926079 , accuracy =  0.9811999797821045
Epoch:  4 , loss =  0.05607683211565018 , accuracy =  0.9822166562080383
Epoch:  5 , loss =  0.05765639990568161 , accuracy =  0.9814500212669373
Epoch:  6 , loss =  0.056580811738967896 , accuracy =  0.9817000031471252
Epoch:  7 , loss =  0.05724235251545906 , accuracy =  0.9815999865531921
ADAGRAD: run no. =  7 , batch size =  64  convergence time =  15.790525674819946
ADAGRAD: run no. =  7 , batch size =  64 , test loss =  0.03834765776991844 , test accuracy =  0.9879999756813049
###### Running part 2: ADAM with batch size  64  ######
Epoch:  1 , loss =  0.03654392808675766 , accuracy =  0.9884833097457886
Epoch:  2 , loss =  0.03389175981283188 , accuracy =  0.989383339881897
Epoch:  3 , loss =  0.03555896878242493 , accuracy =  0.9886000156402588
Epoch:  4 , loss =  0.037866491824388504 , accuracy =  0.988349974155426
Epoch:  5 , loss =  0.034721739590168 , accuracy =  0.9892500042915344
ADAM: run no. =  7 , batch size =  64  convergence time =  11.416146755218506
ADAM: run no. =  7 , batch size =  64 , test loss =  0.03508879244327545 , test accuracy =  0.9908999800682068
###### Running part 2: SGD with batch size  96  ######
Epoch:  1 , loss =  0.036128487437963486 , accuracy =  0.9883999824523926
Epoch:  2 , loss =  0.03547481819987297 , accuracy =  0.9883833527565002
Epoch:  3 , loss =  0.037775520235300064 , accuracy =  0.9876833558082581
Epoch:  4 , loss =  0.03620224446058273 , accuracy =  0.9876333475112915
Epoch:  5 , loss =  0.03534126281738281 , accuracy =  0.9885500073432922
Epoch:  6 , loss =  0.036496616899967194 , accuracy =  0.9881166815757751
Epoch:  7 , loss =  0.03838905319571495 , accuracy =  0.9868833422660828
Epoch:  8 , loss =  0.03671065717935562 , accuracy =  0.988016664981842
SGD: run no. =  7 , batch size =  96  convergence time =  12.731058597564697
SGD: run no. =  7 , batch size =  96 , test loss =  0.026660671457648277 , test accuracy =  0.9911999702453613
###### Running part 2: ADAGRAD with batch size  96  ######
Epoch:  1 , loss =  0.05729671195149422 , accuracy =  0.9821166396141052
Epoch:  2 , loss =  0.05756593123078346 , accuracy =  0.9818166494369507
Epoch:  3 , loss =  0.055972982197999954 , accuracy =  0.9823499917984009
Epoch:  4 , loss =  0.05810686573386192 , accuracy =  0.9819166660308838
Epoch:  5 , loss =  0.05546132102608681 , accuracy =  0.9821666479110718
Epoch:  6 , loss =  0.056384019553661346 , accuracy =  0.9813500046730042
Epoch:  7 , loss =  0.05854625254869461 , accuracy =  0.9815499782562256
Epoch:  8 , loss =  0.05461229383945465 , accuracy =  0.982200026512146
Epoch:  9 , loss =  0.057328809052705765 , accuracy =  0.981166660785675
Epoch:  10 , loss =  0.05709778517484665 , accuracy =  0.9812999963760376
Epoch:  11 , loss =  0.056991346180438995 , accuracy =  0.9816333055496216
ADAGRAD: run no. =  7 , batch size =  96  convergence time =  17.75647759437561
ADAGRAD: run no. =  7 , batch size =  96 , test loss =  0.036854688078165054 , test accuracy =  0.9882000088691711
###### Running part 2: ADAM with batch size  96  ######
Epoch:  1 , loss =  0.03448481857776642 , accuracy =  0.9890000224113464
Epoch:  2 , loss =  0.032198913395404816 , accuracy =  0.9896833300590515
Epoch:  3 , loss =  0.03364798426628113 , accuracy =  0.9890166521072388
Epoch:  4 , loss =  0.03275348246097565 , accuracy =  0.9896000027656555
Epoch:  5 , loss =  0.03245172277092934 , accuracy =  0.9895666837692261
ADAM: run no. =  7 , batch size =  96  convergence time =  8.223241329193115
ADAM: run no. =  7 , batch size =  96 , test loss =  0.0373883955180645 , test accuracy =  0.9907000064849854
###### Running part 2: SGD with batch size  128  ######
Epoch:  1 , loss =  0.03563462570309639 , accuracy =  0.98826664686203
Epoch:  2 , loss =  0.0355471596121788 , accuracy =  0.9878000020980835
Epoch:  3 , loss =  0.03565460443496704 , accuracy =  0.9886833429336548
Epoch:  4 , loss =  0.034903451800346375 , accuracy =  0.9886166453361511
Epoch:  5 , loss =  0.03642652556300163 , accuracy =  0.987766683101654
Epoch:  6 , loss =  0.03479308634996414 , accuracy =  0.9879833459854126
SGD: run no. =  7 , batch size =  128  convergence time =  7.716990947723389
SGD: run no. =  7 , batch size =  128 , test loss =  0.02685508504509926 , test accuracy =  0.991100013256073
###### Running part 2: ADAGRAD with batch size  128  ######
Epoch:  1 , loss =  0.057666752487421036 , accuracy =  0.981416642665863
Epoch:  2 , loss =  0.05821925401687622 , accuracy =  0.9816333055496216
Epoch:  3 , loss =  0.05698937177658081 , accuracy =  0.9817666411399841
Epoch:  4 , loss =  0.05811244621872902 , accuracy =  0.9815499782562256
Epoch:  5 , loss =  0.05523066967725754 , accuracy =  0.9818500280380249
Epoch:  6 , loss =  0.060067467391490936 , accuracy =  0.9814833402633667
Epoch:  7 , loss =  0.055616989731788635 , accuracy =  0.9821166396141052
Epoch:  8 , loss =  0.057110745459795 , accuracy =  0.9819166660308838
ADAGRAD: run no. =  7 , batch size =  128  convergence time =  10.371409177780151
ADAGRAD: run no. =  7 , batch size =  128 , test loss =  0.037235092371702194 , test accuracy =  0.988099992275238
###### Running part 2: ADAM with batch size  128  ######
Epoch:  1 , loss =  0.03286346048116684 , accuracy =  0.9898499846458435
Epoch:  2 , loss =  0.033941175788640976 , accuracy =  0.9894166588783264
Epoch:  3 , loss =  0.032730672508478165 , accuracy =  0.9896500110626221
Epoch:  4 , loss =  0.03177323564887047 , accuracy =  0.9902499914169312
Epoch:  5 , loss =  0.031138325110077858 , accuracy =  0.990149974822998
ADAM: run no. =  7 , batch size =  128  convergence time =  6.5723114013671875
ADAM: run no. =  7 , batch size =  128 , test loss =  0.03675324097275734 , test accuracy =  0.9909999966621399
###### RUN NUMBER  8  ######
###### Running part 2: SGD with batch size  32  ######
Epoch:  1 , loss =  0.03861240670084953 , accuracy =  0.9874333143234253
Epoch:  2 , loss =  0.04007424786686897 , accuracy =  0.9866666793823242
Epoch:  3 , loss =  0.04094619303941727 , accuracy =  0.9866999983787537
Epoch:  4 , loss =  0.039931368082761765 , accuracy =  0.9872333407402039
SGD: run no. =  8 , batch size =  32  convergence time =  12.410417079925537
SGD: run no. =  8 , batch size =  32 , test loss =  0.027899669483304024 , test accuracy =  0.9909999966621399
###### Running part 2: ADAGRAD with batch size  32  ######
Epoch:  1 , loss =  0.06053848937153816 , accuracy =  0.9797833561897278
Epoch:  2 , loss =  0.057615987956523895 , accuracy =  0.9804333448410034
Epoch:  3 , loss =  0.05983050912618637 , accuracy =  0.9810333251953125
Epoch:  4 , loss =  0.05767354369163513 , accuracy =  0.9815166592597961
Epoch:  5 , loss =  0.05942139774560928 , accuracy =  0.980983316898346
ADAGRAD: run no. =  8 , batch size =  32  convergence time =  16.066494703292847
ADAGRAD: run no. =  8 , batch size =  32 , test loss =  0.0385698638856411 , test accuracy =  0.988099992275238
###### Running part 2: ADAM with batch size  32  ######
Epoch:  1 , loss =  0.0398651659488678 , accuracy =  0.9877833127975464
Epoch:  2 , loss =  0.039146773517131805 , accuracy =  0.9882166385650635
Epoch:  3 , loss =  0.039174553006887436 , accuracy =  0.9871000051498413
Epoch:  4 , loss =  0.040252260863780975 , accuracy =  0.9881166815757751
Epoch:  5 , loss =  0.037473924458026886 , accuracy =  0.9883333444595337
Epoch:  6 , loss =  0.04102388024330139 , accuracy =  0.9871666431427002
Epoch:  7 , loss =  0.03585037589073181 , accuracy =  0.9891833066940308
Epoch:  8 , loss =  0.038768041878938675 , accuracy =  0.9879166483879089
Epoch:  9 , loss =  0.038393642753362656 , accuracy =  0.9878666400909424
Epoch:  10 , loss =  0.04027900844812393 , accuracy =  0.9875166416168213
ADAM: run no. =  8 , batch size =  32  convergence time =  32.57170581817627
ADAM: run no. =  8 , batch size =  32 , test loss =  0.03990910202264786 , test accuracy =  0.9904999732971191
###### Running part 2: SGD with batch size  64  ######
Epoch:  1 , loss =  0.037364669144153595 , accuracy =  0.9873999953269958
Epoch:  2 , loss =  0.036363791674375534 , accuracy =  0.9875666499137878
Epoch:  3 , loss =  0.03715341165661812 , accuracy =  0.9872333407402039
Epoch:  4 , loss =  0.03620615229010582 , accuracy =  0.9883999824523926
Epoch:  5 , loss =  0.03472035005688667 , accuracy =  0.9890000224113464
Epoch:  6 , loss =  0.036689840257167816 , accuracy =  0.9879000186920166
Epoch:  7 , loss =  0.035248007625341415 , accuracy =  0.9886333346366882
Epoch:  8 , loss =  0.03469473123550415 , accuracy =  0.9881333112716675
Epoch:  9 , loss =  0.037953443825244904 , accuracy =  0.9872333407402039
Epoch:  10 , loss =  0.03510143607854843 , accuracy =  0.9886833429336548
Epoch:  11 , loss =  0.03619935363531113 , accuracy =  0.9879166483879089
SGD: run no. =  8 , batch size =  64  convergence time =  24.322463989257812
SGD: run no. =  8 , batch size =  64 , test loss =  0.028020357713103294 , test accuracy =  0.991100013256073
###### Running part 2: ADAGRAD with batch size  64  ######
Epoch:  1 , loss =  0.05785784125328064 , accuracy =  0.9818666577339172
Epoch:  2 , loss =  0.054549891501665115 , accuracy =  0.9822999835014343
Epoch:  3 , loss =  0.05596143379807472 , accuracy =  0.9821500182151794
Epoch:  4 , loss =  0.056625932455062866 , accuracy =  0.9817666411399841
Epoch:  5 , loss =  0.057223815470933914 , accuracy =  0.9813833236694336
ADAGRAD: run no. =  8 , batch size =  64  convergence time =  11.64096736907959
ADAGRAD: run no. =  8 , batch size =  64 , test loss =  0.037359315901994705 , test accuracy =  0.9879999756813049
###### Running part 2: ADAM with batch size  64  ######
Epoch:  1 , loss =  0.03610754758119583 , accuracy =  0.98826664686203
Epoch:  2 , loss =  0.03477560356259346 , accuracy =  0.989300012588501
Epoch:  3 , loss =  0.032908935099840164 , accuracy =  0.9895666837692261
Epoch:  4 , loss =  0.03515971451997757 , accuracy =  0.9889500141143799
Epoch:  5 , loss =  0.034785714000463486 , accuracy =  0.9893666505813599
Epoch:  6 , loss =  0.03370797261595726 , accuracy =  0.9893666505813599
ADAM: run no. =  8 , batch size =  64  convergence time =  13.930964469909668
ADAM: run no. =  8 , batch size =  64 , test loss =  0.037026263773441315 , test accuracy =  0.9902999997138977
###### Running part 2: SGD with batch size  96  ######
Epoch:  1 , loss =  0.036644019186496735 , accuracy =  0.9874500036239624
Epoch:  2 , loss =  0.034621674567461014 , accuracy =  0.988183319568634
Epoch:  3 , loss =  0.03478711470961571 , accuracy =  0.988183319568634
Epoch:  4 , loss =  0.035125333815813065 , accuracy =  0.9884166717529297
Epoch:  5 , loss =  0.03285625949501991 , accuracy =  0.9891666769981384
Epoch:  6 , loss =  0.035585884004831314 , accuracy =  0.988183319568634
Epoch:  7 , loss =  0.034597963094711304 , accuracy =  0.9889500141143799
Epoch:  8 , loss =  0.0328371487557888 , accuracy =  0.9891166687011719
Epoch:  9 , loss =  0.03301025554537773 , accuracy =  0.9890833497047424
Epoch:  10 , loss =  0.0358666330575943 , accuracy =  0.9878166913986206
Epoch:  11 , loss =  0.035141266882419586 , accuracy =  0.9886166453361511
SGD: run no. =  8 , batch size =  96  convergence time =  17.412534475326538
SGD: run no. =  8 , batch size =  96 , test loss =  0.02637043409049511 , test accuracy =  0.9919999837875366
###### Running part 2: ADAGRAD with batch size  96  ######
Epoch:  1 , loss =  0.05572512373328209 , accuracy =  0.9820666909217834
Epoch:  2 , loss =  0.05815291777253151 , accuracy =  0.9818833470344543
Epoch:  3 , loss =  0.05572903901338577 , accuracy =  0.9820166826248169
Epoch:  4 , loss =  0.054778870195150375 , accuracy =  0.9825000166893005
Epoch:  5 , loss =  0.05755157768726349 , accuracy =  0.9818166494369507
Epoch:  6 , loss =  0.054059188812971115 , accuracy =  0.9828500151634216
Epoch:  7 , loss =  0.05708523839712143 , accuracy =  0.9819999933242798
Epoch:  8 , loss =  0.05487608164548874 , accuracy =  0.9817500114440918
Epoch:  9 , loss =  0.05571700260043144 , accuracy =  0.9820333123207092
ADAGRAD: run no. =  8 , batch size =  96  convergence time =  14.659970045089722
ADAGRAD: run no. =  8 , batch size =  96 , test loss =  0.037459079176187515 , test accuracy =  0.988099992275238
###### Running part 2: ADAM with batch size  96  ######
Epoch:  1 , loss =  0.03121674433350563 , accuracy =  0.9906499981880188
Epoch:  2 , loss =  0.03221040964126587 , accuracy =  0.9900333285331726
Epoch:  3 , loss =  0.03214336186647415 , accuracy =  0.9898499846458435
Epoch:  4 , loss =  0.032978396862745285 , accuracy =  0.989300012588501
ADAM: run no. =  8 , batch size =  96  convergence time =  6.597888708114624
ADAM: run no. =  8 , batch size =  96 , test loss =  0.039377592504024506 , test accuracy =  0.9901999831199646
###### Running part 2: SGD with batch size  128  ######
Epoch:  1 , loss =  0.034455556422472 , accuracy =  0.9887166619300842
Epoch:  2 , loss =  0.03347599506378174 , accuracy =  0.9891666769981384
Epoch:  3 , loss =  0.03394697606563568 , accuracy =  0.9885833263397217
Epoch:  4 , loss =  0.03408755362033844 , accuracy =  0.9884499907493591
Epoch:  5 , loss =  0.03331412374973297 , accuracy =  0.9885833263397217
Epoch:  6 , loss =  0.033159106969833374 , accuracy =  0.9890000224113464
SGD: run no. =  8 , batch size =  128  convergence time =  7.6511640548706055
SGD: run no. =  8 , batch size =  128 , test loss =  0.02632410265505314 , test accuracy =  0.991599977016449
###### Running part 2: ADAGRAD with batch size  128  ######
Epoch:  1 , loss =  0.05395464599132538 , accuracy =  0.9823166728019714
Epoch:  2 , loss =  0.053725503385066986 , accuracy =  0.9824666380882263
Epoch:  3 , loss =  0.054230738431215286 , accuracy =  0.9824666380882263
Epoch:  4 , loss =  0.05402696877717972 , accuracy =  0.9824833273887634
Epoch:  5 , loss =  0.056078314781188965 , accuracy =  0.9814000129699707
ADAGRAD: run no. =  8 , batch size =  128  convergence time =  6.469832897186279
ADAGRAD: run no. =  8 , batch size =  128 , test loss =  0.036795858293771744 , test accuracy =  0.988099992275238
###### Running part 2: ADAM with batch size  128  ######
Epoch:  1 , loss =  0.03276732563972473 , accuracy =  0.9900166392326355
Epoch:  2 , loss =  0.032029055058956146 , accuracy =  0.9900166392326355
Epoch:  3 , loss =  0.031669702380895615 , accuracy =  0.989633321762085
Epoch:  4 , loss =  0.031812362372875214 , accuracy =  0.9901333451271057
Epoch:  5 , loss =  0.029565265402197838 , accuracy =  0.9902833104133606
Epoch:  6 , loss =  0.030924705788493156 , accuracy =  0.9906499981880188
Epoch:  7 , loss =  0.029392395168542862 , accuracy =  0.9905499815940857
Epoch:  8 , loss =  0.032288890331983566 , accuracy =  0.9905999898910522
Epoch:  9 , loss =  0.03195203095674515 , accuracy =  0.989716649055481
Epoch:  10 , loss =  0.031201334670186043 , accuracy =  0.9897333383560181
ADAM: run no. =  8 , batch size =  128  convergence time =  13.202389240264893
ADAM: run no. =  8 , batch size =  128 , test loss =  0.03530389815568924 , test accuracy =  0.9907000064849854
###### RUN NUMBER  9  ######
###### Running part 2: SGD with batch size  32  ######
Epoch:  1 , loss =  0.03718336299061775 , accuracy =  0.9874666929244995
Epoch:  2 , loss =  0.037003111094236374 , accuracy =  0.9878666400909424
Epoch:  3 , loss =  0.03727569803595543 , accuracy =  0.987333357334137
Epoch:  4 , loss =  0.039305366575717926 , accuracy =  0.986466646194458
Epoch:  5 , loss =  0.0345582440495491 , accuracy =  0.9882500171661377
Epoch:  6 , loss =  0.03581176698207855 , accuracy =  0.988183319568634
Epoch:  7 , loss =  0.03849348798394203 , accuracy =  0.9874500036239624
Epoch:  8 , loss =  0.03554365783929825 , accuracy =  0.988016664981842
SGD: run no. =  9 , batch size =  32  convergence time =  24.711615324020386
SGD: run no. =  9 , batch size =  32 , test loss =  0.027606725692749023 , test accuracy =  0.9914000034332275
###### Running part 2: ADAGRAD with batch size  32  ######
Epoch:  1 , loss =  0.057315971702337265 , accuracy =  0.9816833138465881
Epoch:  2 , loss =  0.060757845640182495 , accuracy =  0.981333315372467
Epoch:  3 , loss =  0.05865702033042908 , accuracy =  0.9816666841506958
Epoch:  4 , loss =  0.05976030230522156 , accuracy =  0.9808499813079834
ADAGRAD: run no. =  9 , batch size =  32  convergence time =  12.968806505203247
ADAGRAD: run no. =  9 , batch size =  32 , test loss =  0.03726852312684059 , test accuracy =  0.9886000156402588
###### Running part 2: ADAM with batch size  32  ######
Epoch:  1 , loss =  0.03781949356198311 , accuracy =  0.9884166717529297
Epoch:  2 , loss =  0.0388965830206871 , accuracy =  0.9884166717529297
Epoch:  3 , loss =  0.03745598718523979 , accuracy =  0.9889500141143799
Epoch:  4 , loss =  0.03674803301692009 , accuracy =  0.9883999824523926
Epoch:  5 , loss =  0.038089435547590256 , accuracy =  0.9883833527565002
Epoch:  6 , loss =  0.0384235717356205 , accuracy =  0.9881333112716675
Epoch:  7 , loss =  0.04163167625665665 , accuracy =  0.9876999855041504
ADAM: run no. =  9 , batch size =  32  convergence time =  22.78697633743286
ADAM: run no. =  9 , batch size =  32 , test loss =  0.04434959590435028 , test accuracy =  0.9894999861717224
###### Running part 2: SGD with batch size  64  ######
Epoch:  1 , loss =  0.03650778904557228 , accuracy =  0.987766683101654
Epoch:  2 , loss =  0.03644151613116264 , accuracy =  0.9878833293914795
Epoch:  3 , loss =  0.034154199063777924 , accuracy =  0.9891833066940308
Epoch:  4 , loss =  0.03404327854514122 , accuracy =  0.9888333082199097
Epoch:  5 , loss =  0.03495956212282181 , accuracy =  0.9886333346366882
Epoch:  6 , loss =  0.03302033245563507 , accuracy =  0.9886000156402588
Epoch:  7 , loss =  0.03385075554251671 , accuracy =  0.9891999959945679
Epoch:  8 , loss =  0.03300725296139717 , accuracy =  0.9891999959945679
Epoch:  9 , loss =  0.03447472304105759 , accuracy =  0.9887333512306213
Epoch:  10 , loss =  0.03324044123291969 , accuracy =  0.9887166619300842
Epoch:  11 , loss =  0.03540363907814026 , accuracy =  0.9883166551589966
SGD: run no. =  9 , batch size =  64  convergence time =  24.32645034790039
SGD: run no. =  9 , batch size =  64 , test loss =  0.026501070708036423 , test accuracy =  0.9916999936103821
###### Running part 2: ADAGRAD with batch size  64  ######
Epoch:  1 , loss =  0.05539210885763168 , accuracy =  0.9821166396141052
Epoch:  2 , loss =  0.05830584838986397 , accuracy =  0.981083333492279
Epoch:  3 , loss =  0.0565401129424572 , accuracy =  0.9815499782562256
Epoch:  4 , loss =  0.05589551478624344 , accuracy =  0.9816499948501587
ADAGRAD: run no. =  9 , batch size =  64  convergence time =  9.072676420211792
ADAGRAD: run no. =  9 , batch size =  64 , test loss =  0.036515012383461 , test accuracy =  0.9884999990463257
###### Running part 2: ADAM with batch size  64  ######
Epoch:  1 , loss =  0.03431236743927002 , accuracy =  0.9890999794006348
Epoch:  2 , loss =  0.03140578791499138 , accuracy =  0.9899333119392395
Epoch:  3 , loss =  0.031828805804252625 , accuracy =  0.989466667175293
Epoch:  4 , loss =  0.034152042120695114 , accuracy =  0.9893166422843933
Epoch:  5 , loss =  0.0349675789475441 , accuracy =  0.9885166883468628
ADAM: run no. =  9 , batch size =  64  convergence time =  11.512472152709961
ADAM: run no. =  9 , batch size =  64 , test loss =  0.038159988820552826 , test accuracy =  0.9904999732971191
###### Running part 2: SGD with batch size  96  ######
Epoch:  1 , loss =  0.03436585143208504 , accuracy =  0.9884999990463257
Epoch:  2 , loss =  0.03452921286225319 , accuracy =  0.9891166687011719
Epoch:  3 , loss =  0.03292463719844818 , accuracy =  0.9887499809265137
Epoch:  4 , loss =  0.03437546640634537 , accuracy =  0.9885333180427551
Epoch:  5 , loss =  0.032803986221551895 , accuracy =  0.9890999794006348
Epoch:  6 , loss =  0.032759781926870346 , accuracy =  0.9887166619300842
Epoch:  7 , loss =  0.03274063393473625 , accuracy =  0.9891499876976013
SGD: run no. =  9 , batch size =  96  convergence time =  11.247736930847168
SGD: run no. =  9 , batch size =  96 , test loss =  0.02657410316169262 , test accuracy =  0.9923999905586243
###### Running part 2: ADAGRAD with batch size  96  ######
Epoch:  1 , loss =  0.055451083928346634 , accuracy =  0.9822666645050049
Epoch:  2 , loss =  0.05412527918815613 , accuracy =  0.9820500016212463
Epoch:  3 , loss =  0.05612660199403763 , accuracy =  0.9818999767303467
Epoch:  4 , loss =  0.05452600494027138 , accuracy =  0.9823333621025085
Epoch:  5 , loss =  0.05423237383365631 , accuracy =  0.9824833273887634
ADAGRAD: run no. =  9 , batch size =  96  convergence time =  8.119735479354858
ADAGRAD: run no. =  9 , batch size =  96 , test loss =  0.03717118874192238 , test accuracy =  0.9884999990463257
###### Running part 2: ADAM with batch size  96  ######
Epoch:  1 , loss =  0.031195566058158875 , accuracy =  0.9901333451271057
Epoch:  2 , loss =  0.03333364799618721 , accuracy =  0.989216685295105
Epoch:  3 , loss =  0.032976891845464706 , accuracy =  0.9900000095367432
Epoch:  4 , loss =  0.03087572567164898 , accuracy =  0.9897333383560181
Epoch:  5 , loss =  0.032989103347063065 , accuracy =  0.9897500276565552
Epoch:  6 , loss =  0.03376689925789833 , accuracy =  0.9892833232879639
Epoch:  7 , loss =  0.032308924943208694 , accuracy =  0.9897666573524475
ADAM: run no. =  9 , batch size =  96  convergence time =  11.627163171768188
ADAM: run no. =  9 , batch size =  96 , test loss =  0.037907250225543976 , test accuracy =  0.9905999898910522
###### Running part 2: SGD with batch size  128  ######
Epoch:  1 , loss =  0.03374175727367401 , accuracy =  0.9890333414077759
Epoch:  2 , loss =  0.032964807003736496 , accuracy =  0.9890999794006348
Epoch:  3 , loss =  0.0318557508289814 , accuracy =  0.9896500110626221
Epoch:  4 , loss =  0.03175148740410805 , accuracy =  0.989383339881897
Epoch:  5 , loss =  0.031707510352134705 , accuracy =  0.989549994468689
Epoch:  6 , loss =  0.03152044117450714 , accuracy =  0.9890666604042053
SGD: run no. =  9 , batch size =  128  convergence time =  7.764376401901245
SGD: run no. =  9 , batch size =  128 , test loss =  0.02681795321404934 , test accuracy =  0.991100013256073
###### Running part 2: ADAGRAD with batch size  128  ######
Epoch:  1 , loss =  0.05737539753317833 , accuracy =  0.9817666411399841
Epoch:  2 , loss =  0.054286934435367584 , accuracy =  0.982699990272522
Epoch:  3 , loss =  0.05534572899341583 , accuracy =  0.9824166893959045
Epoch:  4 , loss =  0.056803204119205475 , accuracy =  0.9822666645050049
Epoch:  5 , loss =  0.05397826433181763 , accuracy =  0.9828000068664551
Epoch:  6 , loss =  0.05571655556559563 , accuracy =  0.9821833372116089
Epoch:  7 , loss =  0.05341644585132599 , accuracy =  0.9823333621025085
Epoch:  8 , loss =  0.05608794093132019 , accuracy =  0.9817333221435547
Epoch:  9 , loss =  0.05650991573929787 , accuracy =  0.9817833304405212
Epoch:  10 , loss =  0.05289521440863609 , accuracy =  0.9827499985694885
ADAGRAD: run no. =  9 , batch size =  128  convergence time =  13.119914531707764
ADAGRAD: run no. =  9 , batch size =  128 , test loss =  0.037209123373031616 , test accuracy =  0.9879999756813049
###### Running part 2: ADAM with batch size  128  ######
Epoch:  1 , loss =  0.03003840148448944 , accuracy =  0.9902999997138977
Epoch:  2 , loss =  0.03241308405995369 , accuracy =  0.9900333285331726
Epoch:  3 , loss =  0.029530225321650505 , accuracy =  0.9902666807174683
Epoch:  4 , loss =  0.03235083445906639 , accuracy =  0.989799976348877
Epoch:  5 , loss =  0.03103477507829666 , accuracy =  0.9906499981880188
Epoch:  6 , loss =  0.030645867809653282 , accuracy =  0.9902499914169312
ADAM: run no. =  9 , batch size =  128  convergence time =  8.021744012832642
ADAM: run no. =  9 , batch size =  128 , test loss =  0.03744547441601753 , test accuracy =  0.9902999997138977